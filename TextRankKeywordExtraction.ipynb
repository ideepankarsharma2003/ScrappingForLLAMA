{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **[Text Rank](https://analyticsindiamag.com/guide-to-nlps-textrank-algorithm/#:~:text=Textrank%20is%20a%20graph-based%20ranking%20algorithm%20like%20Google%E2%80%99s,measure%20the%20relationship%20between%20two%20or%20more%20words.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = '''Not only did it only confirm that the film would be unfunny and generic, but it\n",
    "also managed to give away the ENTIRE movie; and I'm not exaggerating - every moment, every \n",
    "plot point, every joke is told in the trailer.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "en_nlp.add_pipe(\"textrank\")\n",
    "doc = en_nlp(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.00798225402832\n"
     ]
    }
   ],
   "source": [
    "tr = doc._.textrank\n",
    "print(tr.elapsed_time);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTIRE 0.12192569955977017 1\n",
      "the trailer 0.09398909652862131 1\n",
      "the ENTIRE movie 0.08809272122598891 1\n",
      "every joke 0.06774199896077908 1\n",
      "the film 0.03642647504614921 1\n",
      "I 0.0 1\n",
      "it 0.0 2\n"
     ]
    }
   ],
   "source": [
    "for combination in doc._.phrases:\n",
    "    print(combination.text, combination.rank, combination.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_cleaner import clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention                                vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention By Woosuk Kwon*, Zhuohan Li*, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Yu, Joey Gonzalez, Hao Zhang, and Ion Stoica (* Equal Contribution). June 20th, 2023   |  | Paper (Stay Tuned)   LLMs promise to fundamentally change how we use AI across all industries. However, actually serving these models is challenging and can be surprisingly slow even on expensive hardware. Today we are excited to introduce vLLM, an open-source library for fast LLM inference and serving. vLLM utilizes PagedAttention, our new attention algorithm that effectively manages attention keys and values. vLLM equipped with PagedAttention redefines the new state of the art in LLM serving: it delivers up to 24x higher throughput than HuggingFace Transformers, without requiring any model architecture changes. vLLM has been developed at UC Berkeley and deployed at  for the past two months. It is the core technology that makes LLM serving affordable even for a small research team like LMSYS with limited compute resources. Try out vLLM now with a single command at our . Beyond State-of-the-art Performance We compare the throughput of vLLM with , the most popular LLM library and , the previous state of the art. We evaluate in two settings: LLaMA-7B on an NVIDIA A10G GPU and LLaMA-13B on an NVIDIA A100 GPU (40GB). We sample the requests’ input/output lengths from the ShareGPT dataset. In our experiments, vLLM achieves up to 24x higher throughput compared to HF and up to 3.5x higher throughput than TGI.       Serving throughput when each request asks for  one output completion. vLLM achieves 14x - 24x higher throughput than HF and 2.2x - 2.5x higher throughput than TGI.        Serving throughput when each request asks for three parallel output completions. vLLM achieves 8.5x - 15x higher throughput than HF and 3.3x - 3.5x higher throughput than TGI.  The Secret Sauce: PagedAttention In vLLM, we identify that the performance of LLM serving is bottlenecked by memory. In the autoregressive decoding process, all the input tokens to the LLM produce their attention key and value tensors, and these tensors are kept in GPU memory to generate next tokens. These cached key and value tensors are often referred to as KV cache. The KV cache is  Large: Takes up to 1.7GB for a single sequence in LLaMA-13B. Dynamic: Its size depends on the sequence length, which is highly variable and unpredictable. As a result, efficiently managing the KV cache presents a significant challenge. We find that existing systems waste 60% – 80% of memory due to fragmentation and over-reservation.  To address this problem, we introduce PagedAttention, an attention algorithm inspired by the classic idea of virtual memory and paging in operating systems. Unlike the traditional attention algorithms, PagedAttention allows storing continuous keys and values in non-contiguous memory space. Specifically, PagedAttention partitions the KV cache of each sequence into blocks, each block containing the keys and values for a fixed number of tokens. During the attention computation, the PagedAttention kernel identifies and fetches these blocks efficiently.      PagedAttention: KV Cache are partitioned into blocks. Blocks do not need to be contiguous in memory space.  Because the blocks do not need to be contiguous in memory, we can manage the keys and values in a more flexible way as in OS’s virtual memory: one can think of blocks as pages, tokens as bytes, and sequences as processes. The contiguous logical blocks of a sequence are mapped to non-contiguous physical blocks via a block table. The physical blocks are allocated on demand as new tokens are generated.      Example generation process for a request with PagedAttention.  In PagedAttention, memory waste only happens in the last block of a sequence. In practice, this results in near-optimal memory usage, with a mere waste of under 4%. This boost in memory efficiency proves highly beneficial: It allows the system to batch more sequences together, increase GPU utilization, and thereby significantly increase the throughput as shown in the performance result above. PagedAttention has another key advantage: efficient memory sharing. For example, in parallel sampling, multiple output sequences are generated from the same prompt. In this case, the computation and memory for the prompt can be shared between the output sequences.      Example of parallel sampling.  PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the Copy-on-Write mechanism.      Example generation process for a request that samples multiple outputs.  PageAttention’s memory sharing greatly reduces the memory overhead of complex sampling algorithms, such as parallel sampling and beam search, cutting their memory usage by up to 55%. This can translate into up to 2.2x improvement in throughput. This makes such sampling methods practical in LLM services. PagedAttention is the core technology behind vLLM, our LLM inference and serving engine that supports a variety of models with high performance and an easy-to-use interface. For more technical details about vLLM and PagedAttention, check out our  and stay tuned for our paper. The Silent Hero Behind LMSYS Vicuna and Chatbot Arena This April,  developed the popular Vicuna chatbot models and made them publicly available. Since then, Vicuna has been served in  for millions of users. Initially, LMSYS FastChat adopted a HF Transformers based  to serve the chat demo. As the demo became more popular, the peak traffic ramped up several times, making the HF backend a significant bottleneck. The LMSYS and vLLM team have worked together and soon developed the FastChat-vLLM integration to use vLLM  in order to support the growing demands (up to 5x more traffic). In an early  by LMSYS, the vLLM serving backend can achieve up to 30x higher throughput than an initial HF backend. Since mid-April, the most popular models such as Vicuna, Koala, and LLaMA, have all been successfully served using the FastChat-vLLM integration – With FastChat as the multi-model chat serving frontend and vLLM as the inference backend, LMSYS is able to harness a limited number of university-sponsored GPUs to serve Vicuna to millions of users with high throughput and low latency. LMSYS is expanding the use of vLLM to a wider range of models, including Databricks Dolly, LAION’s OpenAsssiant, and Stability AI’s stableLM. The  is being developed and forthcoming.      Requests served by FastChat-vLLM integration in the Chatbot Arena between April to May. Indeed, more than half of the requests to Chatbot Arena use vLLM as the inference backend.  This utilization of vLLM has also significantly reduced operational costs. With vLLM, LMSYS was able to cut the number of GPUs used for serving the above traffic by 50%. vLLM has been handling an average of 30K requests daily and a peak of 60K, which is a clear demonstration of vLLM’s robustness. Get started with vLLM Install vLLM with the following command (check out our  for more): $ pip install vllm  vLLM can be used for both offline inference and online serving. To use vLLM for offline inference, you can import vLLM and use the LLM class in your Python scripts: from vllm import LLM  prompts = [\"Hello, my name is\", \"The capital of France is\"]  # Sample prompts. llm = LLM(model=\"lmsys/vicuna-7b-v1.3\")  # Create an LLM. outputs = llm.generate(prompts)  # Generate texts from the prompts.  To use vLLM for online serving, you can start an OpenAI API-compatible server via: $ python -m vllm.entrypoints.openai.api_server --model lmsys/vicuna-7b-v1.3  You can query the server with the same format as OpenAI API: $ curl http://localhost:8000/v1/completions \\\\     -H \"Content-Type: application/json\" \\\\     -d \\'{         \"model\": \"lmsys/vicuna-7b-v1.3\",         \"prompt\": \"San Francisco is a\",         \"max_tokens\": 7,         \"temperature\": 0     }\\'  For more ways to use vLLM, please check out the .   Blog written by Woosuk Kwon and Zhuohan Li (UC Berkeley). Special thanks to Hao Zhang for the integration of vLLM and FastChat and for writing the corresponding section. We thank the entire team\\u200a—\\u200aSiyuan Zhuang, Ying Sheng, Lianmin Zheng (UC Berkeley), Cody Yu (Independent Researcher), Joey Gonzalez (UC Berkeley), Hao Zhang (UC Berkeley & UCSD), and Ion Stoica (UC Berkeley).                © 2023. vLLM Team. All rights reserved.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url= \"https://vllm.ai/\"\n",
    "text= clean(url)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8793\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "en_nlp.add_pipe(\"textrank\", config={ \"stopwords\": { \"word\": [\"NOUN\"] } })\n",
    "doc = en_nlp(text)\n",
    "# for phrase in doc._.phrases[:10]:\n",
    "#     print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase(text='high throughput', chunks=[high throughput], count=1, rank=0.07562172157618913)\n",
      "Phrase(text='higher throughput', chunks=[higher throughput], count=1, rank=0.07562172157618913)\n",
      "Phrase(text='24x higher throughput', chunks=[24x higher throughput, 24x higher throughput], count=2, rank=0.07297240370281138)\n",
      "Phrase(text='GPU memory', chunks=[GPU memory], count=1, rank=0.0727322163949857)\n",
      "Phrase(text='30x higher throughput', chunks=[30x higher throughput], count=1, rank=0.07162601585390109)\n",
      "Phrase(text='multiple output sequences', chunks=[multiple output sequences], count=1, rank=0.07105415954145451)\n",
      "Phrase(text='non-contiguous physical blocks', chunks=[non-contiguous physical blocks], count=1, rank=0.07082531727539203)\n",
      "Phrase(text='more sequences', chunks=[more sequences], count=1, rank=0.06991381465916642)\n",
      "Phrase(text='memory sharing', chunks=[memory sharing], count=1, rank=0.06930227586847752)\n",
      "Phrase(text='Blocks', chunks=[Blocks], count=1, rank=0.06928679226211538)\n",
      "Phrase(text='blocks', chunks=[blocks, blocks, blocks], count=3, rank=0.06928679226211538)\n",
      "Phrase(text='memory waste', chunks=[memory waste], count=1, rank=0.0686396210246226)\n",
      "Phrase(text='virtual memory', chunks=[virtual memory], count=1, rank=0.06833762260734297)\n",
      "Phrase(text='memory space', chunks=[memory space], count=1, rank=0.06799664361896138)\n",
      "Phrase(text='memory', chunks=[memory, memory, memory, memory], count=4, rank=0.06745865416021882)\n",
      "Phrase(text='non-contiguous memory space', chunks=[non-contiguous memory space], count=1, rank=0.06661031783556397)\n",
      "Phrase(text='sequences', chunks=[sequences], count=1, rank=0.06595173319486126)\n",
      "Phrase(text='different sequences', chunks=[different sequences], count=1, rank=0.0655430588467959)\n",
      "Phrase(text='memory efficiency', chunks=[memory efficiency], count=1, rank=0.06548629706492681)\n",
      "Phrase(text='efficient memory sharing', chunks=[efficient memory sharing], count=1, rank=0.06543714529219223)\n",
      "Phrase(text='serving engine', chunks=[serving engine], count=1, rank=0.06267518288901579)\n",
      "Phrase(text='throughput', chunks=[throughput, throughput, throughput], count=3, rank=0.06043331334444307)\n",
      "Phrase(text='fast LLM inference', chunks=[fast LLM inference], count=1, rank=0.06022511430133219)\n",
      "Phrase(text='UC Berkeley', chunks=[UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley], count=11, rank=0.059878337309280584)\n",
      "Phrase(text='high performance', chunks=[high performance], count=1, rank=0.05793026755145428)\n",
      "Phrase(text='Cheap LLM', chunks=[Cheap LLM, Cheap LLM], count=2, rank=0.05553238845545522)\n",
      "Phrase(text='vLLM utilizes PagedAttention', chunks=[vLLM utilizes PagedAttention], count=1, rank=0.05542051400120708)\n",
      "Phrase(text='LLM services', chunks=[LLM services], count=1, rank=0.0553974090450809)\n",
      "Phrase(text='LLM', chunks=[LLM, LLM, LLM, LLM, LLM, LLM, LLM, LLM, LLM, LLM, LLM], count=11, rank=0.05470279373719321)\n",
      "Phrase(text='new tokens', chunks=[new tokens], count=1, rank=0.04982630418085735)\n",
      "Phrase(text='models', chunks=[models, models], count=2, rank=0.04794074486824392)\n",
      "Phrase(text='vLLM achieves 14x - 24x higher throughput', chunks=[vLLM achieves 14x - 24x higher throughput], count=1, rank=0.0478744905704505)\n",
      "Phrase(text='next tokens', chunks=[next tokens], count=1, rank=0.04781612244017727)\n",
      "Phrase(text='vLLM achieves 8.5x - 15x higher throughput', chunks=[vLLM achieves 8.5x - 15x higher throughput], count=1, rank=0.04773030888930461)\n",
      "Phrase(text='attention keys', chunks=[attention keys], count=1, rank=0.04703671195605914)\n",
      "Phrase(text='HF Transformers', chunks=[HF Transformers], count=1, rank=0.04703010073460162)\n",
      "Phrase(text='PagedAttention', chunks=[PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention, PagedAttention], count=28, rank=0.04651568925722762)\n",
      "Phrase(text='multiple outputs', chunks=[multiple outputs], count=1, rank=0.04601410739666177)\n",
      "Phrase(text='the same physical block', chunks=[the same physical block], count=1, rank=0.04478224697321962)\n",
      "Phrase(text='physical pages', chunks=[physical pages], count=1, rank=0.04474225080650163)\n",
      "Phrase(text='tokens', chunks=[tokens, tokens], count=2, rank=0.04472287580996138)\n",
      "Phrase(text='vLLM', chunks=[vLLM, vLLM, vLLM, vLLM, vLLM, vLLM, vLLM, vLLM, vLLM, vLLM], count=10, rank=0.04310345213394317)\n",
      "Phrase(text='HF', chunks=[HF, HF, HF, HF, HF, HF, HF], count=7, rank=0.04300226912973275)\n",
      "Phrase(text='more traffic', chunks=[more traffic], count=1, rank=0.042947665755931015)\n",
      "Phrase(text='vLLM Team', chunks=[vLLM Team], count=1, rank=0.04249777716255582)\n",
      "Phrase(text='Hao Zhang', chunks=[Hao Zhang, Hao Zhang, Hao Zhang, Hao Zhang, Hao Zhang], count=5, rank=0.04229083940032165)\n",
      "Phrase(text='GPU utilization', chunks=[GPU utilization], count=1, rank=0.04165880316129835)\n",
      "Phrase(text='The contiguous logical blocks', chunks=[The contiguous logical blocks], count=1, rank=0.04128456112036941)\n",
      "Phrase(text='continuous keys', chunks=[continuous keys], count=1, rank=0.04071875245415511)\n",
      "Phrase(text='3.5x higher throughput', chunks=[3.5x higher throughput], count=1, rank=0.04010447409864329)\n",
      "Phrase(text='operating systems', chunks=[operating systems], count=1, rank=0.03973336588338827)\n",
      "Phrase(text='near-optimal memory usage', chunks=[near-optimal memory usage], count=1, rank=0.03973187730726359)\n",
      "Phrase(text='Joey Gonzalez', chunks=[Joey Gonzalez, Joey Gonzalez, Joey Gonzalez], count=3, rank=0.03967632727580567)\n",
      "Phrase(text='the output sequences', chunks=[the output sequences], count=1, rank=0.03951992375009039)\n",
      "Phrase(text='The physical blocks', chunks=[The physical blocks], count=1, rank=0.03951015853987336)\n",
      "Phrase(text='the physical blocks', chunks=[the physical blocks], count=1, rank=0.03951015853987336)\n",
      "Phrase(text='A10G GPU', chunks=[A10G GPU], count=1, rank=0.03936459081221031)\n",
      "Phrase(text='values', chunks=[values, values, values, values], count=4, rank=0.039279936612823266)\n",
      "Phrase(text='Generate', chunks=[Generate], count=1, rank=0.03924243158732306)\n",
      "Phrase(text='Ion Stoica', chunks=[Ion Stoica, Ion Stoica, Ion Stoica], count=3, rank=0.039152737552595906)\n",
      "Phrase(text='the multi-model chat serving frontend', chunks=[the multi-model chat serving frontend], count=1, rank=0.038883486089181137)\n",
      "Phrase(text='OpenAI API', chunks=[OpenAI API], count=1, rank=0.03878664067301772)\n",
      "Phrase(text='OS’s virtual memory', chunks=[OS’s virtual memory], count=1, rank=0.03841464726760175)\n",
      "Phrase(text='offline inference', chunks=[offline inference], count=1, rank=0.03812805269760859)\n",
      "Phrase(text='PageAttention’s memory sharing', chunks=[PageAttention’s memory sharing], count=1, rank=0.038045887395362195)\n",
      "Phrase(text='more ways', chunks=[more ways], count=1, rank=0.03793838784154862)\n",
      "Phrase(text='Cody Yu', chunks=[Cody Yu, Cody Yu, Cody Yu], count=3, rank=0.03777699155847539)\n",
      "Phrase(text='LMSYS FastChat', chunks=[LMSYS FastChat], count=1, rank=0.03776281883232095)\n",
      "Phrase(text='LMSYS', chunks=[LMSYS, LMSYS, LMSYS, LMSYS, LMSYS, LMSYS, LMSYS, LMSYS, LMSYS], count=9, rank=0.03771795418695949)\n",
      "Phrase(text='GPU', chunks=[GPU, GPU, GPU], count=3, rank=0.0374239521340932)\n",
      "Phrase(text='LMSYS Vicuna', chunks=[LMSYS Vicuna, LMSYS Vicuna], count=2, rank=0.03733815966010013)\n",
      "Phrase(text='a block table', chunks=[a block table], count=1, rank=0.03731677649579268)\n",
      "Phrase(text='its block table', chunks=[its block table], count=1, rank=0.03731677649579268)\n",
      "Phrase(text='complex sampling algorithms', chunks=[complex sampling algorithms], count=1, rank=0.0371306380969528)\n",
      "Phrase(text='their logical blocks', chunks=[their logical blocks], count=1, rank=0.03667784297251894)\n",
      "Phrase(text='the last block', chunks=[the last block], count=1, rank=0.03646525586833501)\n",
      "Phrase(text='their memory usage', chunks=[their memory usage], count=1, rank=0.03634230374438926)\n",
      "Phrase(text='vLLM team', chunks=[vLLM team], count=1, rank=0.03618619786226199)\n",
      "Phrase(text='processes', chunks=[processes, processes], count=2, rank=0.03597993985744325)\n",
      "Phrase(text='KV cache', chunks=[KV cache], count=1, rank=0.035945875697010715)\n",
      "Phrase(text='limited compute resources', chunks=[limited compute resources], count=1, rank=0.03576691288653101)\n",
      "Phrase(text='existing systems', chunks=[existing systems], count=1, rank=0.0356706081628697)\n",
      "Phrase(text='the sequence length', chunks=[the sequence length], count=1, rank=0.03541236960401603)\n",
      "Phrase(text='parallel sampling', chunks=[parallel sampling, parallel sampling, parallel sampling], count=3, rank=0.03530270099432261)\n",
      "Phrase(text='a single sequence', chunks=[a single sequence], count=1, rank=0.035152230827812304)\n",
      "Phrase(text='Lianmin Zheng', chunks=[Lianmin Zheng, Lianmin Zheng, Lianmin Zheng], count=3, rank=0.034756363539004326)\n",
      "Phrase(text='Example generation process', chunks=[Example generation process, Example generation process], count=2, rank=0.034745726733153195)\n",
      "Phrase(text='more technical details', chunks=[more technical details], count=1, rank=0.034683118465302566)\n",
      "Phrase(text='San Francisco', chunks=[San Francisco, San Francisco], count=2, rank=0.03457286363565794)\n",
      "Phrase(text='UC Berkeley & UCSD', chunks=[UC Berkeley & UCSD], count=1, rank=0.03441559725103332)\n",
      "Phrase(text='the popular Vicuna chatbot models', chunks=[the popular Vicuna chatbot models], count=1, rank=0.03426563723764006)\n",
      "Phrase(text='expensive hardware', chunks=[expensive hardware], count=1, rank=0.03413958412082102)\n",
      "Phrase(text='Zhuohan Li', chunks=[Zhuohan Li, Zhuohan Li, Zhuohan Li], count=3, rank=0.033654137970847084)\n",
      "Phrase(text='Ying Sheng', chunks=[Ying Sheng, Ying Sheng, Ying Sheng], count=3, rank=0.033605460621318)\n",
      "Phrase(text='such sampling methods', chunks=[such sampling methods], count=1, rank=0.03333359237169007)\n",
      "Phrase(text='Requests', chunks=[Requests], count=1, rank=0.03320103952393506)\n",
      "Phrase(text='low latency', chunks=[low latency], count=1, rank=0.03309354303720878)\n",
      "Phrase(text='Siyuan Zhuang', chunks=[Siyuan Zhuang, Siyuan Zhuang, Siyuan Zhuang], count=3, rank=0.03242560444231096)\n",
      "Phrase(text='our LLM inference', chunks=[our LLM inference], count=1, rank=0.0324070831239747)\n",
      "Phrase(text='Chatbot Arena', chunks=[Chatbot Arena, Chatbot Arena, Chatbot Arena, Chatbot Arena], count=4, rank=0.03234196418764754)\n",
      "Phrase(text='TGI', chunks=[TGI, TGI, TGI], count=3, rank=0.03221610930031038)\n",
      "Phrase(text='the blocks', chunks=[the blocks, the blocks], count=2, rank=0.0320021985310544)\n",
      "Phrase(text='these blocks', chunks=[these blocks], count=1, rank=0.0320021985310544)\n",
      "Phrase(text='FastChat', chunks=[FastChat, FastChat], count=2, rank=0.03197840501885609)\n",
      "Phrase(text='Woosuk Kwon', chunks=[Woosuk Kwon, Woosuk Kwon, Woosuk Kwon, Woosuk Kwon], count=4, rank=0.03173619992032588)\n",
      "Phrase(text='AI', chunks=[AI, AI], count=2, rank=0.03157078902518677)\n",
      "Phrase(text='vLLM Install', chunks=[vLLM Install], count=1, rank=0.031420948240879445)\n",
      "Phrase(text='Vicuna', chunks=[Vicuna, Vicuna, Vicuna, Vicuna, Vicuna, Vicuna, Vicuna], count=7, rank=0.031412410418025706)\n",
      "Phrase(text='2.2x - 2.5x higher throughput', chunks=[2.2x - 2.5x higher throughput], count=1, rank=0.0312922232421788)\n",
      "Phrase(text='the memory', chunks=[the memory], count=1, rank=0.031157817710857766)\n",
      "Phrase(text='three parallel output completions', chunks=[three parallel output completions], count=1, rank=0.03107994897528015)\n",
      "Phrase(text='FastChat-vLLM integration', chunks=[FastChat-vLLM integration], count=1, rank=0.030667142532322195)\n",
      "Phrase(text='a sequence', chunks=[a sequence, a sequence], count=2, rank=0.030461800730860425)\n",
      "Phrase(text='each sequence', chunks=[each sequence], count=1, rank=0.030461800730860425)\n",
      "Phrase(text='safe sharing', chunks=[safe sharing], count=1, rank=0.030252310714035273)\n",
      "Phrase(text='any model architecture changes', chunks=[any model architecture changes], count=1, rank=0.030251978545728697)\n",
      "Phrase(text='fragmentation', chunks=[fragmentation], count=1, rank=0.030241397302695226)\n",
      "Phrase(text='HuggingFace Transformers', chunks=[HuggingFace Transformers], count=1, rank=0.030180676477016083)\n",
      "Phrase(text='Independent Researcher', chunks=[Independent Researcher, Independent Researcher], count=2, rank=0.030154823582648555)\n",
      "Phrase(text='an initial HF backend', chunks=[an initial HF backend], count=1, rank=0.0296841142647742)\n",
      "Phrase(text='the LLM class', chunks=[the LLM class], count=1, rank=0.028722985922112178)\n",
      "Phrase(text='the same prompt', chunks=[the same prompt], count=1, rank=0.02857973393088281)\n",
      "Phrase(text='pages', chunks=[pages], count=1, rank=0.02843504358578868)\n",
      "Phrase(text='bytes', chunks=[bytes], count=1, rank=0.02818341720955667)\n",
      "Phrase(text='KV Cache', chunks=[KV Cache], count=1, rank=0.028018458116350765)\n",
      "Phrase(text='GPUs', chunks=[GPUs], count=1, rank=0.027972294695775238)\n",
      "Phrase(text='the throughput', chunks=[the throughput, the throughput], count=2, rank=0.02791295178194817)\n",
      "Phrase(text='operational costs', chunks=[operational costs], count=1, rank=0.027844990222987004)\n",
      "Phrase(text='GB', chunks=[GB, GB], count=2, rank=0.027812430036000677)\n",
      "Phrase(text='one output completion', chunks=[one output completion], count=1, rank=0.027738144068709232)\n",
      "Phrase(text='their attention key and value tensors', chunks=[their attention key and value tensors], count=1, rank=0.026823976013435495)\n",
      "Phrase(text='demand', chunks=[demand], count=1, rank=0.0267151692063795)\n",
      "Phrase(text='an NVIDIA A10G GPU', chunks=[an NVIDIA A10G GPU], count=1, rank=0.026219778854991677)\n",
      "Phrase(text='KV', chunks=[KV, KV, KV, KV], count=4, rank=0.02621413674403791)\n",
      "Phrase(text='Stability AI’s stableLM', chunks=[Stability AI’s stableLM], count=1, rank=0.02593994569540383)\n",
      "Phrase(text='mid-April', chunks=[mid-April], count=1, rank=0.02588633912681951)\n",
      "Phrase(text='24x', chunks=[24x, 24x], count=2, rank=0.025816407547135066)\n",
      "Phrase(text='State', chunks=[State], count=1, rank=0.025599460354184853)\n",
      "Phrase(text='an LLM', chunks=[an LLM], count=1, rank=0.025266138151674458)\n",
      "Phrase(text='the LLM', chunks=[the LLM], count=1, rank=0.025266138151674458)\n",
      "Phrase(text='an NVIDIA A100 GPU', chunks=[an NVIDIA A100 GPU], count=1, rank=0.025241721586585653)\n",
      "Phrase(text='the most popular LLM library', chunks=[the most popular LLM library], count=1, rank=0.025183959760417568)\n",
      "Phrase(text='*', chunks=[*], count=1, rank=0.02511291065546224)\n",
      "Phrase(text='beam search', chunks=[beam search], count=1, rank=0.024994793766128683)\n",
      "Phrase(text='a HF Transformers', chunks=[a HF Transformers], count=1, rank=0.02494147736199243)\n",
      "Phrase(text='users', chunks=[users, users], count=2, rank=0.02440828681637927)\n",
      "Phrase(text='These cached key and value tensors', chunks=[These cached key and value tensors], count=1, rank=0.024374520211866032)\n",
      "Phrase(text='Write', chunks=[Write], count=1, rank=0.02432818019558906)\n",
      "Phrase(text='use', chunks=[use], count=1, rank=0.024142268677311614)\n",
      "Phrase(text='April', chunks=[April, April], count=2, rank=0.024048675120338064)\n",
      "Phrase(text='OpenAsssiant', chunks=[OpenAsssiant], count=1, rank=0.024042595760931162)\n",
      "Phrase(text='NVIDIA', chunks=[NVIDIA, NVIDIA], count=2, rank=0.023954461892423944)\n",
      "Phrase(text='the inference backend', chunks=[the inference backend, the inference backend], count=2, rank=0.023950113242452605)\n",
      "Phrase(text='vLLM’s robustness', chunks=[vLLM’s robustness], count=1, rank=0.023918149055853592)\n",
      "Phrase(text='our new attention algorithm', chunks=[our new attention algorithm], count=1, rank=0.02336025232075054)\n",
      "Phrase(text='LAION', chunks=[LAION], count=1, rank=0.02303415431707261)\n",
      "Phrase(text='Databricks', chunks=[Databricks], count=1, rank=0.022834037110181764)\n",
      "Phrase(text='the FastChat-vLLM integration', chunks=[the FastChat-vLLM integration, the FastChat-vLLM integration], count=2, rank=0.022678657742613212)\n",
      "Phrase(text='the same format', chunks=[the same format], count=1, rank=0.02264748383821921)\n",
      "Phrase(text='50%', chunks=[50%, 50%], count=2, rank=0.022619748534606656)\n",
      "Phrase(text='80%', chunks=[80%], count=1, rank=0.022619748534606656)\n",
      "Phrase(text='UCSD', chunks=[UCSD], count=1, rank=0.022373347279421953)\n",
      "Phrase(text='ShareGPT', chunks=[ShareGPT], count=1, rank=0.022365060692424697)\n",
      "Phrase(text='millions', chunks=[millions, millions, millions, millions], count=4, rank=0.022267788784610842)\n",
      "Phrase(text='these models', chunks=[these models], count=1, rank=0.02214288156386544)\n",
      "Phrase(text='the traditional attention algorithms', chunks=[the traditional attention algorithms], count=1, rank=0.021495152486714807)\n",
      "Phrase(text='university-sponsored GPUs', chunks=[university-sponsored GPUs], count=1, rank=0.021326106927576943)\n",
      "Phrase(text='an OpenAI API-compatible server', chunks=[an OpenAI API-compatible server], count=1, rank=0.021279151886754166)\n",
      "Phrase(text='LLaMA-13B', chunks=[LLaMA-13B, LLaMA-13B], count=2, rank=0.021111079944519225)\n",
      "Phrase(text='the most popular models', chunks=[the most popular models], count=1, rank=0.020586327684026903)\n",
      "Phrase(text='3.3x', chunks=[3.3x], count=1, rank=0.02051699971129998)\n",
      "Phrase(text='the autoregressive decoding process', chunks=[the autoregressive decoding process], count=1, rank=0.020505847007904938)\n",
      "Phrase(text='60% – 80%', chunks=[60% – 80%], count=1, rank=0.02040549672727624)\n",
      "Phrase(text='both offline inference', chunks=[both offline inference], count=1, rank=0.020220453461937803)\n",
      "Phrase(text='the requests’ input/output lengths', chunks=[the requests’ input/output lengths], count=1, rank=0.02020929094670484)\n",
      "Phrase(text='an attention algorithm', chunks=[an attention algorithm], count=1, rank=0.020191199326071968)\n",
      "Phrase(text='30x', chunks=[30x], count=1, rank=0.020171915093913386)\n",
      "Phrase(text='Stability AI’s', chunks=[Stability AI’s], count=1, rank=0.019976555908326886)\n",
      "Phrase(text='30K requests', chunks=[30K requests], count=1, rank=0.019887225498907982)\n",
      "Phrase(text='the HF', chunks=[the HF], count=1, rank=0.01986189732625275)\n",
      "Phrase(text='a limited number', chunks=[a limited number], count=1, rank=0.01980184269431209)\n",
      "Phrase(text='texts', chunks=[texts], count=1, rank=0.01979022857361993)\n",
      "Phrase(text='the prompt', chunks=[the prompt], count=1, rank=0.019781760538028794)\n",
      "Phrase(text='the prompts', chunks=[the prompts], count=1, rank=0.019781760538028794)\n",
      "Phrase(text='\\\\     -H \"Content-Type: application/json\" \\\\', chunks=[\\     -H \"Content-Type: application/json\" \\], count=1, rank=0.019767046217550804)\n",
      "Phrase(text='a significant bottleneck', chunks=[a significant bottleneck], count=1, rank=0.019663807987631243)\n",
      "Phrase(text='the peak traffic', chunks=[the peak traffic], count=1, rank=0.019425942572304594)\n",
      "Phrase(text='a small research team', chunks=[a small research team], count=1, rank=0.019326530628281925)\n",
      "Phrase(text='The KV cache', chunks=[The KV cache], count=1, rank=0.019063179345783745)\n",
      "Phrase(text='the KV cache', chunks=[the KV cache, the KV cache], count=2, rank=0.019063179345783745)\n",
      "Phrase(text='LLaMA', chunks=[LLaMA], count=1, rank=0.01905862353899243)\n",
      "Phrase(text='Koala', chunks=[Koala, Koala], count=2, rank=0.019055845796028165)\n",
      "Phrase(text='the new state', chunks=[the new state], count=1, rank=0.019027545240886908)\n",
      "Phrase(text='a significant challenge', chunks=[a significant challenge], count=1, rank=0.01900945956703393)\n",
      "Phrase(text='* Equal Contribution', chunks=[* Equal Contribution], count=1, rank=0.018985489164410892)\n",
      "Phrase(text='a fixed number', chunks=[a fixed number], count=1, rank=0.018781968122596453)\n",
      "Phrase(text='track', chunks=[track], count=1, rank=0.01876242986504907)\n",
      "Phrase(text=']  # Sample prompts', chunks=[]  # Sample prompts], count=1, rank=0.018570334448878574)\n",
      "Phrase(text='more than half', chunks=[more than half, more than half], count=2, rank=0.01836021579587357)\n",
      "Phrase(text='Paper (Stay Tuned', chunks=[Paper (Stay Tuned], count=1, rank=0.018255505010340962)\n",
      "Phrase(text='Performance', chunks=[Performance], count=1, rank=0.018068163154029017)\n",
      "Phrase(text='May', chunks=[May], count=1, rank=0.01806643168164917)\n",
      "Phrase(text='the above traffic', chunks=[the above traffic], count=1, rank=0.01801541504635542)\n",
      "Phrase(text='the keys', chunks=[the keys, the keys], count=2, rank=0.01793600777700507)\n",
      "Phrase(text='a mere waste', chunks=[a mere waste], count=1, rank=0.017777294169626038)\n",
      "Phrase(text='the attention computation', chunks=[the attention computation], count=1, rank=0.017372470661738984)\n",
      "Phrase(text='order', chunks=[order], count=1, rank=0.017299602239495476)\n",
      "Phrase(text='June 20th', chunks=[June 20th], count=1, rank=0.017248050196797794)\n",
      "Phrase(text='the Chatbot Arena', chunks=[the Chatbot Arena, the Chatbot Arena], count=2, rank=0.017151916645483534)\n",
      "Phrase(text='Blog', chunks=[Blog, Blog], count=2, rank=0.017146702667510304)\n",
      "Phrase(text='#', chunks=[#, #], count=2, rank=0.017078976956404597)\n",
      "Phrase(text='the chat demo', chunks=[the chat demo], count=1, rank=0.016917176634540516)\n",
      "Phrase(text='a single command', chunks=[a single command], count=1, rank=0.0167992078579403)\n",
      "Phrase(text='Special thanks', chunks=[Special thanks], count=1, rank=0.01673668747635632)\n",
      "Phrase(text='LAION’s OpenAsssiant', chunks=[LAION’s OpenAsssiant], count=1, rank=0.016647962908613322)\n",
      "Phrase(text='under 4%', chunks=[under 4%, under 4%], count=2, rank=0.0164261080068621)\n",
      "Phrase(text='the system', chunks=[the system], count=1, rank=0.016358156900763667)\n",
      "Phrase(text='the growing demands', chunks=[the growing demands], count=1, rank=0.0163135941278161)\n",
      "Phrase(text='-m vllm.entrypoints.openai.api_server --model lmsys/vicuna-7b-v1.3', chunks=[-m vllm.entrypoints.openai.api_server --model lmsys/vicuna-7b-v1.3], count=1, rank=0.016242184617889167)\n",
      "Phrase(text='the previous state', chunks=[the previous state], count=1, rank=0.01593189987226156)\n",
      "Phrase(text='a request', chunks=[a request, a request], count=2, rank=0.015334903285215449)\n",
      "Phrase(text='each request', chunks=[each request, each request], count=2, rank=0.015334903285215449)\n",
      "Phrase(text='the requests', chunks=[the requests], count=1, rank=0.015334903285215449)\n",
      "Phrase(text='The Secret Sauce: PagedAttention In', chunks=[The Secret Sauce: PagedAttention In], count=1, rank=0.015319371325850747)\n",
      "Phrase(text='a wider range', chunks=[a wider range], count=1, rank=0.015186900310613293)\n",
      "Phrase(text='France', chunks=[France, France], count=2, rank=0.015076337261031055)\n",
      "Phrase(text='14x - 24x', chunks=[14x - 24x], count=1, rank=0.01491369049194056)\n",
      "Phrase(text='3.3x - 3.5x', chunks=[3.3x - 3.5x], count=1, rank=0.014778539038594003)\n",
      "Phrase(text='the performance', chunks=[the performance, the performance], count=2, rank=0.014562896245119012)\n",
      "Phrase(text='| Paper (Stay Tuned)   LLMs', chunks=[| Paper (Stay Tuned)   LLMs], count=1, rank=0.014548640967325234)\n",
      "Phrase(text='vllm  vLLM', chunks=[vllm  vLLM], count=1, rank=0.014457337355460793)\n",
      "Phrase(text='the past two months', chunks=[the past two months, the past two months], count=2, rank=0.014401603783140388)\n",
      "Phrase(text='the following command', chunks=[the following command], count=1, rank=0.014357484738430154)\n",
      "Phrase(text='the corresponding section', chunks=[the corresponding section], count=1, rank=0.014339125943939419)\n",
      "Phrase(text='the number', chunks=[the number], count=1, rank=0.014238968498174473)\n",
      "Phrase(text='the entire team', chunks=[the entire team], count=1, rank=0.014110505094562981)\n",
      "Phrase(text='the server', chunks=[the server], count=1, rank=0.01405200955326575)\n",
      "Phrase(text='LLM(model=\"lmsys', chunks=[LLM(model=\"lmsys], count=1, rank=0.01398713808702859)\n",
      "Phrase(text='the reference counts', chunks=[the reference counts], count=1, rank=0.013972568552958172)\n",
      "Phrase(text='your Python scripts', chunks=[your Python scripts], count=1, rank=0.013905544756477907)\n",
      "Phrase(text='up to 55%', chunks=[up to 55%, up to 55%], count=2, rank=0.013461910371519668)\n",
      "Phrase(text='a clear demonstration', chunks=[a clear demonstration], count=1, rank=0.013359084602305449)\n",
      "Phrase(text='the integration', chunks=[the integration], count=1, rank=0.013310920902430157)\n",
      "Phrase(text='-', chunks=[-], count=1, rank=0.013308512011846999)\n",
      "Phrase(text='2.2x', chunks=[2.2x], count=1, rank=0.013308512011846999)\n",
      "Phrase(text='the classic idea', chunks=[the classic idea], count=1, rank=0.01329753085835121)\n",
      "Phrase(text='Team', chunks=[Team], count=1, rank=0.013187536193880295)\n",
      "Phrase(text='these tensors', chunks=[these tensors], count=1, rank=0.013103805869906752)\n",
      "Phrase(text='40GB', chunks=[40GB], count=1, rank=0.0128460111744821)\n",
      "Phrase(text='the art', chunks=[the art, the art], count=2, rank=0.012802019883114883)\n",
      "Phrase(text='$ curl http://localhost:8000/v1/completions', chunks=[$ curl http://localhost:8000/v1/completions], count=1, rank=0.012560531018262985)\n",
      "Phrase(text='all industries', chunks=[all industries], count=1, rank=0.012559126609432439)\n",
      "Phrase(text='an open-source library', chunks=[an open-source library], count=1, rank=0.012485538957894731)\n",
      "Phrase(text='over-reservation', chunks=[over-reservation], count=1, rank=0.012334748856769436)\n",
      "Phrase(text='Example', chunks=[Example], count=1, rank=0.011747970214973282)\n",
      "Phrase(text='PageAttention', chunks=[PageAttention], count=1, rank=0.011747970214973282)\n",
      "Phrase(text='Today', chunks=[Today], count=1, rank=0.011747970214973282)\n",
      "Phrase(text='example', chunks=[example], count=1, rank=0.011747970214973282)\n",
      "Phrase(text='mid', chunks=[mid], count=1, rank=0.011747970214973282)\n",
      "Phrase(text='practice', chunks=[practice], count=1, rank=0.011747970214973282)\n",
      "Phrase(text='{         \"model', chunks=['{         \"model], count=1, rank=0.0114144630638676)\n",
      "Phrase(text='8.5x - 15x', chunks=[8.5x - 15x], count=1, rank=0.011240313128528297)\n",
      "Phrase(text='a peak', chunks=[a peak], count=1, rank=0.011159042157088387)\n",
      "Phrase(text='the use', chunks=[the use], count=1, rank=0.011150836255755305)\n",
      "Phrase(text='The LMSYS', chunks=[The LMSYS], count=1, rank=0.011139511988190793)\n",
      "Phrase(text='This April', chunks=[This April], count=1, rank=0.011107607243504831)\n",
      "Phrase(text='between April to May', chunks=[between April to May], count=1, rank=0.010915219736366765)\n",
      "Phrase(text='This utilization', chunks=[This utilization], count=1, rank=0.010850112602557969)\n",
      "Phrase(text='Databricks Dolly', chunks=[Databricks Dolly], count=1, rank=0.010546589977799477)\n",
      "Phrase(text='another key advantage', chunks=[another key advantage], count=1, rank=0.010519078989473568)\n",
      "Phrase(text='a more flexible way', chunks=[a more flexible way], count=1, rank=0.010482566713522564)\n",
      "Phrase(text='60K', chunks=[60K], count=1, rank=0.010116103879342905)\n",
      "Phrase(text='the demo', chunks=[the demo], count=1, rank=0.009914615122167458)\n",
      "Phrase(text='$ pip', chunks=[$ pip], count=1, rank=0.00945829480144956)\n",
      "Phrase(text='to 1.7GB', chunks=[to 1.7GB], count=1, rank=0.009328572626993166)\n",
      "Phrase(text='our paper', chunks=[our paper], count=1, rank=0.009316690561112513)\n",
      "Phrase(text='the-art', chunks=[the-art], count=1, rank=0.00929662683846003)\n",
      "Phrase(text='the core technology', chunks=[the core technology, the core technology], count=2, rank=0.009285629710564328)\n",
      "Phrase(text='June 20th,', chunks=[June 20th,], count=1, rank=0.009147159942301265)\n",
      "Phrase(text='a variety', chunks=[a variety], count=1, rank=0.00913399752531888)\n",
      "Phrase(text='The Secret Sauce', chunks=[The Secret Sauce], count=1, rank=0.00887596890684864)\n",
      "Phrase(text='The Silent Hero', chunks=[The Silent Hero], count=1, rank=0.00887596890684864)\n",
      "Phrase(text='up to 2.2x improvement', chunks=[up to 2.2x improvement], count=1, rank=0.007520635327310399)\n",
      "Phrase(text='the computation', chunks=[the computation], count=1, rank=0.006946233890810962)\n",
      "Phrase(text='an average', chunks=[an average], count=1, rank=0.006900214608725471)\n",
      "Phrase(text='2.2x - 2.5x', chunks=[2.2x - 2.5x], count=1, rank=0.006881610907321546)\n",
      "Phrase(text='all the input', chunks=[all the input], count=1, rank=0.0068547409110543255)\n",
      "Phrase(text='a\",         \"max_tokens\": 7,         \"temperature', chunks=[a\",         \"max_tokens\": 7,         \"temperature], count=1, rank=0.0066951172349367745)\n",
      "Phrase(text='The capital', chunks=[The capital], count=1, rank=0.006146938394042587)\n",
      "Phrase(text='this problem', chunks=[this problem], count=1, rank=0.006146938394042587)\n",
      "Phrase(text='two settings', chunks=[two settings], count=1, rank=0.006146938394042587)\n",
      "Phrase(text='All rights', chunks=[All rights], count=1, rank=0.005426155012837224)\n",
      "Phrase(text='Its size', chunks=[Its size], count=1, rank=0.005426155012837224)\n",
      "Phrase(text='This boost', chunks=[This boost], count=1, rank=0.005426155012837224)\n",
      "Phrase(text='a result', chunks=[a result], count=1, rank=0.005426155012837224)\n",
      "Phrase(text='my name', chunks=[my name], count=1, rank=0.005426155012837224)\n",
      "Phrase(text='our experiments', chunks=[our experiments], count=1, rank=0.005426155012837224)\n",
      "Phrase(text='this case', chunks=[this case], count=1, rank=0.005426155012837224)\n",
      "Phrase(text='$', chunks=[$], count=1, rank=0.0)\n",
      "Phrase(text='2023', chunks=[2023, 2023], count=2, rank=0.0)\n",
      "Phrase(text='30', chunks=[30], count=1, rank=0.0)\n",
      "Phrase(text='40', chunks=[40], count=1, rank=0.0)\n",
      "Phrase(text='60', chunks=[60], count=1, rank=0.0)\n",
      "Phrase(text='7', chunks=[7], count=1, rank=0.0)\n",
      "Phrase(text='It', chunks=[It, It], count=2, rank=0.0)\n",
      "Phrase(text='LLaMA-7B', chunks=[LLaMA-7B], count=1, rank=0.0)\n",
      "Phrase(text='The', chunks=[The], count=1, rank=0.0)\n",
      "Phrase(text='This', chunks=[This, This], count=2, rank=0.0)\n",
      "Phrase(text='We', chunks=[We, We, We, We, We], count=5, rank=0.0)\n",
      "Phrase(text='You', chunks=[You], count=1, rank=0.0)\n",
      "Phrase(text='daily', chunks=[daily], count=1, rank=0.0)\n",
      "Phrase(text='http://localhost:8000', chunks=[http://localhost:8000], count=1, rank=0.0)\n",
      "Phrase(text='it', chunks=[it], count=1, rank=0.0)\n",
      "Phrase(text='one', chunks=[one, one], count=2, rank=0.0)\n",
      "Phrase(text='our', chunks=[our, our, our], count=3, rank=0.0)\n",
      "Phrase(text='that', chunks=[that, that, that, that], count=4, rank=0.0)\n",
      "Phrase(text='the', chunks=[the], count=1, rank=0.0)\n",
      "Phrase(text='them', chunks=[them], count=1, rank=0.0)\n",
      "Phrase(text='this', chunks=[this], count=1, rank=0.0)\n",
      "Phrase(text='three', chunks=[three], count=1, rank=0.0)\n",
      "Phrase(text='two', chunks=[two], count=1, rank=0.0)\n",
      "Phrase(text='up to 5x', chunks=[up to 5x], count=1, rank=0.0)\n",
      "Phrase(text='we', chunks=[we, we, we, we, we], count=5, rank=0.0)\n",
      "Phrase(text='which', chunks=[which, which], count=2, rank=0.0)\n",
      "Phrase(text='you', chunks=[you, you], count=2, rank=0.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for phrase in doc._.phrases:\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "en_nlp.add_pipe(\"textrank\", config={ \"stopwords\": { \"word\": [\"NOUN\"] } })\n",
    "doc = en_nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-7f587792aa8142e6b3feedf5280ebc1d.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-7f587792aa8142e6b3feedf5280ebc1d.vega-embed details,\n",
       "  #altair-viz-7f587792aa8142e6b3feedf5280ebc1d.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-7f587792aa8142e6b3feedf5280ebc1d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-7f587792aa8142e6b3feedf5280ebc1d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-7f587792aa8142e6b3feedf5280ebc1d\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.8.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.8.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-ad1f26d500b3ca8f2ba233445a87ca5f\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"count\", \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"text\", \"type\": \"nominal\"}, {\"field\": \"rank\", \"type\": \"quantitative\"}, {\"field\": \"count\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"index\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"rank\", \"type\": \"quantitative\"}}, \"title\": \"Keyphrase profile of the document\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.8.0.json\", \"datasets\": {\"data-ad1f26d500b3ca8f2ba233445a87ca5f\": [{\"index\": 0, \"text\": \"high throughput\", \"count\": 1, \"rank\": 0.07562172157618913}, {\"index\": 1, \"text\": \"higher throughput\", \"count\": 1, \"rank\": 0.07562172157618913}, {\"index\": 2, \"text\": \"24x higher throughput\", \"count\": 2, \"rank\": 0.07297240370281138}, {\"index\": 3, \"text\": \"GPU memory\", \"count\": 1, \"rank\": 0.0727322163949857}, {\"index\": 4, \"text\": \"30x higher throughput\", \"count\": 1, \"rank\": 0.07162601585390109}, {\"index\": 5, \"text\": \"multiple output sequences\", \"count\": 1, \"rank\": 0.07105415954145451}, {\"index\": 6, \"text\": \"non-contiguous physical blocks\", \"count\": 1, \"rank\": 0.07082531727539203}, {\"index\": 7, \"text\": \"more sequences\", \"count\": 1, \"rank\": 0.06991381465916642}, {\"index\": 8, \"text\": \"memory sharing\", \"count\": 1, \"rank\": 0.06930227586847752}, {\"index\": 9, \"text\": \"Blocks\", \"count\": 1, \"rank\": 0.06928679226211538}, {\"index\": 10, \"text\": \"blocks\", \"count\": 3, \"rank\": 0.06928679226211538}, {\"index\": 11, \"text\": \"memory waste\", \"count\": 1, \"rank\": 0.0686396210246226}, {\"index\": 12, \"text\": \"virtual memory\", \"count\": 1, \"rank\": 0.06833762260734297}, {\"index\": 13, \"text\": \"memory space\", \"count\": 1, \"rank\": 0.06799664361896138}, {\"index\": 14, \"text\": \"memory\", \"count\": 4, \"rank\": 0.06745865416021882}, {\"index\": 15, \"text\": \"non-contiguous memory space\", \"count\": 1, \"rank\": 0.06661031783556397}, {\"index\": 16, \"text\": \"sequences\", \"count\": 1, \"rank\": 0.06595173319486126}, {\"index\": 17, \"text\": \"different sequences\", \"count\": 1, \"rank\": 0.0655430588467959}, {\"index\": 18, \"text\": \"memory efficiency\", \"count\": 1, \"rank\": 0.06548629706492681}, {\"index\": 19, \"text\": \"efficient memory sharing\", \"count\": 1, \"rank\": 0.06543714529219223}, {\"index\": 20, \"text\": \"serving engine\", \"count\": 1, \"rank\": 0.06267518288901579}, {\"index\": 21, \"text\": \"throughput\", \"count\": 3, \"rank\": 0.06043331334444307}, {\"index\": 22, \"text\": \"fast LLM inference\", \"count\": 1, \"rank\": 0.06022511430133219}, {\"index\": 23, \"text\": \"UC Berkeley\", \"count\": 11, \"rank\": 0.059878337309280584}, {\"index\": 24, \"text\": \"high performance\", \"count\": 1, \"rank\": 0.05793026755145428}, {\"index\": 25, \"text\": \"Cheap LLM\", \"count\": 2, \"rank\": 0.05553238845545522}, {\"index\": 26, \"text\": \"vLLM utilizes PagedAttention\", \"count\": 1, \"rank\": 0.05542051400120708}, {\"index\": 27, \"text\": \"LLM services\", \"count\": 1, \"rank\": 0.0553974090450809}, {\"index\": 28, \"text\": \"LLM\", \"count\": 11, \"rank\": 0.05470279373719321}, {\"index\": 29, \"text\": \"new tokens\", \"count\": 1, \"rank\": 0.04982630418085735}, {\"index\": 30, \"text\": \"models\", \"count\": 2, \"rank\": 0.04794074486824392}, {\"index\": 31, \"text\": \"vLLM achieves 14x - 24x higher throughput\", \"count\": 1, \"rank\": 0.0478744905704505}, {\"index\": 32, \"text\": \"next tokens\", \"count\": 1, \"rank\": 0.04781612244017727}, {\"index\": 33, \"text\": \"vLLM achieves 8.5x - 15x higher throughput\", \"count\": 1, \"rank\": 0.04773030888930461}, {\"index\": 34, \"text\": \"attention keys\", \"count\": 1, \"rank\": 0.04703671195605914}, {\"index\": 35, \"text\": \"HF Transformers\", \"count\": 1, \"rank\": 0.04703010073460162}, {\"index\": 36, \"text\": \"PagedAttention\", \"count\": 28, \"rank\": 0.04651568925722762}, {\"index\": 37, \"text\": \"multiple outputs\", \"count\": 1, \"rank\": 0.04601410739666177}, {\"index\": 38, \"text\": \"the same physical block\", \"count\": 1, \"rank\": 0.04478224697321962}, {\"index\": 39, \"text\": \"physical pages\", \"count\": 1, \"rank\": 0.04474225080650163}, {\"index\": 40, \"text\": \"tokens\", \"count\": 2, \"rank\": 0.04472287580996138}, {\"index\": 41, \"text\": \"vLLM\", \"count\": 10, \"rank\": 0.04310345213394317}, {\"index\": 42, \"text\": \"HF\", \"count\": 7, \"rank\": 0.04300226912973275}, {\"index\": 43, \"text\": \"more traffic\", \"count\": 1, \"rank\": 0.042947665755931015}, {\"index\": 44, \"text\": \"vLLM Team\", \"count\": 1, \"rank\": 0.04249777716255582}, {\"index\": 45, \"text\": \"Hao Zhang\", \"count\": 5, \"rank\": 0.04229083940032165}, {\"index\": 46, \"text\": \"GPU utilization\", \"count\": 1, \"rank\": 0.04165880316129835}, {\"index\": 47, \"text\": \"The contiguous logical blocks\", \"count\": 1, \"rank\": 0.04128456112036941}, {\"index\": 48, \"text\": \"continuous keys\", \"count\": 1, \"rank\": 0.04071875245415511}, {\"index\": 49, \"text\": \"3.5x higher throughput\", \"count\": 1, \"rank\": 0.04010447409864329}, {\"index\": 50, \"text\": \"operating systems\", \"count\": 1, \"rank\": 0.03973336588338827}, {\"index\": 51, \"text\": \"near-optimal memory usage\", \"count\": 1, \"rank\": 0.03973187730726359}, {\"index\": 52, \"text\": \"Joey Gonzalez\", \"count\": 3, \"rank\": 0.03967632727580567}, {\"index\": 53, \"text\": \"the output sequences\", \"count\": 1, \"rank\": 0.03951992375009039}, {\"index\": 54, \"text\": \"The physical blocks\", \"count\": 1, \"rank\": 0.03951015853987336}, {\"index\": 55, \"text\": \"the physical blocks\", \"count\": 1, \"rank\": 0.03951015853987336}, {\"index\": 56, \"text\": \"A10G GPU\", \"count\": 1, \"rank\": 0.03936459081221031}, {\"index\": 57, \"text\": \"values\", \"count\": 4, \"rank\": 0.039279936612823266}, {\"index\": 58, \"text\": \"Generate\", \"count\": 1, \"rank\": 0.03924243158732306}, {\"index\": 59, \"text\": \"Ion Stoica\", \"count\": 3, \"rank\": 0.039152737552595906}, {\"index\": 60, \"text\": \"the multi-model chat serving frontend\", \"count\": 1, \"rank\": 0.038883486089181137}, {\"index\": 61, \"text\": \"OpenAI API\", \"count\": 1, \"rank\": 0.03878664067301772}, {\"index\": 62, \"text\": \"OS\\u2019s virtual memory\", \"count\": 1, \"rank\": 0.03841464726760175}, {\"index\": 63, \"text\": \"offline inference\", \"count\": 1, \"rank\": 0.03812805269760859}, {\"index\": 64, \"text\": \"PageAttention\\u2019s memory sharing\", \"count\": 1, \"rank\": 0.038045887395362195}, {\"index\": 65, \"text\": \"more ways\", \"count\": 1, \"rank\": 0.03793838784154862}, {\"index\": 66, \"text\": \"Cody Yu\", \"count\": 3, \"rank\": 0.03777699155847539}, {\"index\": 67, \"text\": \"LMSYS FastChat\", \"count\": 1, \"rank\": 0.03776281883232095}, {\"index\": 68, \"text\": \"LMSYS\", \"count\": 9, \"rank\": 0.03771795418695949}, {\"index\": 69, \"text\": \"GPU\", \"count\": 3, \"rank\": 0.0374239521340932}, {\"index\": 70, \"text\": \"LMSYS Vicuna\", \"count\": 2, \"rank\": 0.03733815966010013}, {\"index\": 71, \"text\": \"a block table\", \"count\": 1, \"rank\": 0.03731677649579268}, {\"index\": 72, \"text\": \"its block table\", \"count\": 1, \"rank\": 0.03731677649579268}, {\"index\": 73, \"text\": \"complex sampling algorithms\", \"count\": 1, \"rank\": 0.0371306380969528}, {\"index\": 74, \"text\": \"their logical blocks\", \"count\": 1, \"rank\": 0.03667784297251894}, {\"index\": 75, \"text\": \"the last block\", \"count\": 1, \"rank\": 0.03646525586833501}, {\"index\": 76, \"text\": \"their memory usage\", \"count\": 1, \"rank\": 0.03634230374438926}, {\"index\": 77, \"text\": \"vLLM team\", \"count\": 1, \"rank\": 0.03618619786226199}, {\"index\": 78, \"text\": \"processes\", \"count\": 2, \"rank\": 0.03597993985744325}, {\"index\": 79, \"text\": \"KV cache\", \"count\": 1, \"rank\": 0.035945875697010715}, {\"index\": 80, \"text\": \"limited compute resources\", \"count\": 1, \"rank\": 0.03576691288653101}, {\"index\": 81, \"text\": \"existing systems\", \"count\": 1, \"rank\": 0.0356706081628697}, {\"index\": 82, \"text\": \"the sequence length\", \"count\": 1, \"rank\": 0.03541236960401603}, {\"index\": 83, \"text\": \"parallel sampling\", \"count\": 3, \"rank\": 0.03530270099432261}, {\"index\": 84, \"text\": \"a single sequence\", \"count\": 1, \"rank\": 0.035152230827812304}, {\"index\": 85, \"text\": \"Lianmin Zheng\", \"count\": 3, \"rank\": 0.034756363539004326}, {\"index\": 86, \"text\": \"Example generation process\", \"count\": 2, \"rank\": 0.034745726733153195}, {\"index\": 87, \"text\": \"more technical details\", \"count\": 1, \"rank\": 0.034683118465302566}, {\"index\": 88, \"text\": \"San Francisco\", \"count\": 2, \"rank\": 0.03457286363565794}, {\"index\": 89, \"text\": \"UC Berkeley & UCSD\", \"count\": 1, \"rank\": 0.03441559725103332}, {\"index\": 90, \"text\": \"the popular Vicuna chatbot models\", \"count\": 1, \"rank\": 0.03426563723764006}, {\"index\": 91, \"text\": \"expensive hardware\", \"count\": 1, \"rank\": 0.03413958412082102}, {\"index\": 92, \"text\": \"Zhuohan Li\", \"count\": 3, \"rank\": 0.033654137970847084}, {\"index\": 93, \"text\": \"Ying Sheng\", \"count\": 3, \"rank\": 0.033605460621318}, {\"index\": 94, \"text\": \"such sampling methods\", \"count\": 1, \"rank\": 0.03333359237169007}, {\"index\": 95, \"text\": \"Requests\", \"count\": 1, \"rank\": 0.03320103952393506}, {\"index\": 96, \"text\": \"low latency\", \"count\": 1, \"rank\": 0.03309354303720878}, {\"index\": 97, \"text\": \"Siyuan Zhuang\", \"count\": 3, \"rank\": 0.03242560444231096}, {\"index\": 98, \"text\": \"our LLM inference\", \"count\": 1, \"rank\": 0.0324070831239747}, {\"index\": 99, \"text\": \"Chatbot Arena\", \"count\": 4, \"rank\": 0.03234196418764754}, {\"index\": 100, \"text\": \"TGI\", \"count\": 3, \"rank\": 0.03221610930031038}, {\"index\": 101, \"text\": \"the blocks\", \"count\": 2, \"rank\": 0.0320021985310544}, {\"index\": 102, \"text\": \"these blocks\", \"count\": 1, \"rank\": 0.0320021985310544}, {\"index\": 103, \"text\": \"FastChat\", \"count\": 2, \"rank\": 0.03197840501885609}, {\"index\": 104, \"text\": \"Woosuk Kwon\", \"count\": 4, \"rank\": 0.03173619992032588}, {\"index\": 105, \"text\": \"AI\", \"count\": 2, \"rank\": 0.03157078902518677}, {\"index\": 106, \"text\": \"vLLM Install\", \"count\": 1, \"rank\": 0.031420948240879445}, {\"index\": 107, \"text\": \"Vicuna\", \"count\": 7, \"rank\": 0.031412410418025706}, {\"index\": 108, \"text\": \"2.2x - 2.5x higher throughput\", \"count\": 1, \"rank\": 0.0312922232421788}, {\"index\": 109, \"text\": \"the memory\", \"count\": 1, \"rank\": 0.031157817710857766}, {\"index\": 110, \"text\": \"three parallel output completions\", \"count\": 1, \"rank\": 0.03107994897528015}, {\"index\": 111, \"text\": \"FastChat-vLLM integration\", \"count\": 1, \"rank\": 0.030667142532322195}, {\"index\": 112, \"text\": \"a sequence\", \"count\": 2, \"rank\": 0.030461800730860425}, {\"index\": 113, \"text\": \"each sequence\", \"count\": 1, \"rank\": 0.030461800730860425}, {\"index\": 114, \"text\": \"safe sharing\", \"count\": 1, \"rank\": 0.030252310714035273}, {\"index\": 115, \"text\": \"any model architecture changes\", \"count\": 1, \"rank\": 0.030251978545728697}, {\"index\": 116, \"text\": \"fragmentation\", \"count\": 1, \"rank\": 0.030241397302695226}, {\"index\": 117, \"text\": \"HuggingFace Transformers\", \"count\": 1, \"rank\": 0.030180676477016083}, {\"index\": 118, \"text\": \"Independent Researcher\", \"count\": 2, \"rank\": 0.030154823582648555}, {\"index\": 119, \"text\": \"an initial HF backend\", \"count\": 1, \"rank\": 0.0296841142647742}, {\"index\": 120, \"text\": \"the LLM class\", \"count\": 1, \"rank\": 0.028722985922112178}, {\"index\": 121, \"text\": \"the same prompt\", \"count\": 1, \"rank\": 0.02857973393088281}, {\"index\": 122, \"text\": \"pages\", \"count\": 1, \"rank\": 0.02843504358578868}, {\"index\": 123, \"text\": \"bytes\", \"count\": 1, \"rank\": 0.02818341720955667}, {\"index\": 124, \"text\": \"KV Cache\", \"count\": 1, \"rank\": 0.028018458116350765}, {\"index\": 125, \"text\": \"GPUs\", \"count\": 1, \"rank\": 0.027972294695775238}, {\"index\": 126, \"text\": \"the throughput\", \"count\": 2, \"rank\": 0.02791295178194817}, {\"index\": 127, \"text\": \"operational costs\", \"count\": 1, \"rank\": 0.027844990222987004}, {\"index\": 128, \"text\": \"GB\", \"count\": 2, \"rank\": 0.027812430036000677}, {\"index\": 129, \"text\": \"one output completion\", \"count\": 1, \"rank\": 0.027738144068709232}, {\"index\": 130, \"text\": \"their attention key and value tensors\", \"count\": 1, \"rank\": 0.026823976013435495}, {\"index\": 131, \"text\": \"demand\", \"count\": 1, \"rank\": 0.0267151692063795}, {\"index\": 132, \"text\": \"an NVIDIA A10G GPU\", \"count\": 1, \"rank\": 0.026219778854991677}, {\"index\": 133, \"text\": \"KV\", \"count\": 4, \"rank\": 0.02621413674403791}, {\"index\": 134, \"text\": \"Stability AI\\u2019s stableLM\", \"count\": 1, \"rank\": 0.02593994569540383}, {\"index\": 135, \"text\": \"mid-April\", \"count\": 1, \"rank\": 0.02588633912681951}, {\"index\": 136, \"text\": \"24x\", \"count\": 2, \"rank\": 0.025816407547135066}, {\"index\": 137, \"text\": \"State\", \"count\": 1, \"rank\": 0.025599460354184853}, {\"index\": 138, \"text\": \"an LLM\", \"count\": 1, \"rank\": 0.025266138151674458}, {\"index\": 139, \"text\": \"the LLM\", \"count\": 1, \"rank\": 0.025266138151674458}, {\"index\": 140, \"text\": \"an NVIDIA A100 GPU\", \"count\": 1, \"rank\": 0.025241721586585653}, {\"index\": 141, \"text\": \"the most popular LLM library\", \"count\": 1, \"rank\": 0.025183959760417568}, {\"index\": 142, \"text\": \"*\", \"count\": 1, \"rank\": 0.02511291065546224}, {\"index\": 143, \"text\": \"beam search\", \"count\": 1, \"rank\": 0.024994793766128683}, {\"index\": 144, \"text\": \"a HF Transformers\", \"count\": 1, \"rank\": 0.02494147736199243}, {\"index\": 145, \"text\": \"users\", \"count\": 2, \"rank\": 0.02440828681637927}, {\"index\": 146, \"text\": \"These cached key and value tensors\", \"count\": 1, \"rank\": 0.024374520211866032}, {\"index\": 147, \"text\": \"Write\", \"count\": 1, \"rank\": 0.02432818019558906}, {\"index\": 148, \"text\": \"use\", \"count\": 1, \"rank\": 0.024142268677311614}, {\"index\": 149, \"text\": \"April\", \"count\": 2, \"rank\": 0.024048675120338064}, {\"index\": 150, \"text\": \"OpenAsssiant\", \"count\": 1, \"rank\": 0.024042595760931162}, {\"index\": 151, \"text\": \"NVIDIA\", \"count\": 2, \"rank\": 0.023954461892423944}, {\"index\": 152, \"text\": \"the inference backend\", \"count\": 2, \"rank\": 0.023950113242452605}, {\"index\": 153, \"text\": \"vLLM\\u2019s robustness\", \"count\": 1, \"rank\": 0.023918149055853592}, {\"index\": 154, \"text\": \"our new attention algorithm\", \"count\": 1, \"rank\": 0.02336025232075054}, {\"index\": 155, \"text\": \"LAION\", \"count\": 1, \"rank\": 0.02303415431707261}, {\"index\": 156, \"text\": \"Databricks\", \"count\": 1, \"rank\": 0.022834037110181764}, {\"index\": 157, \"text\": \"the FastChat-vLLM integration\", \"count\": 2, \"rank\": 0.022678657742613212}, {\"index\": 158, \"text\": \"the same format\", \"count\": 1, \"rank\": 0.02264748383821921}, {\"index\": 159, \"text\": \"50%\", \"count\": 2, \"rank\": 0.022619748534606656}, {\"index\": 160, \"text\": \"80%\", \"count\": 1, \"rank\": 0.022619748534606656}, {\"index\": 161, \"text\": \"UCSD\", \"count\": 1, \"rank\": 0.022373347279421953}, {\"index\": 162, \"text\": \"ShareGPT\", \"count\": 1, \"rank\": 0.022365060692424697}, {\"index\": 163, \"text\": \"millions\", \"count\": 4, \"rank\": 0.022267788784610842}, {\"index\": 164, \"text\": \"these models\", \"count\": 1, \"rank\": 0.02214288156386544}, {\"index\": 165, \"text\": \"the traditional attention algorithms\", \"count\": 1, \"rank\": 0.021495152486714807}, {\"index\": 166, \"text\": \"university-sponsored GPUs\", \"count\": 1, \"rank\": 0.021326106927576943}, {\"index\": 167, \"text\": \"an OpenAI API-compatible server\", \"count\": 1, \"rank\": 0.021279151886754166}, {\"index\": 168, \"text\": \"LLaMA-13B\", \"count\": 2, \"rank\": 0.021111079944519225}, {\"index\": 169, \"text\": \"the most popular models\", \"count\": 1, \"rank\": 0.020586327684026903}, {\"index\": 170, \"text\": \"3.3x\", \"count\": 1, \"rank\": 0.02051699971129998}, {\"index\": 171, \"text\": \"the autoregressive decoding process\", \"count\": 1, \"rank\": 0.020505847007904938}, {\"index\": 172, \"text\": \"60% \\u2013 80%\", \"count\": 1, \"rank\": 0.02040549672727624}, {\"index\": 173, \"text\": \"both offline inference\", \"count\": 1, \"rank\": 0.020220453461937803}, {\"index\": 174, \"text\": \"the requests\\u2019 input/output lengths\", \"count\": 1, \"rank\": 0.02020929094670484}, {\"index\": 175, \"text\": \"an attention algorithm\", \"count\": 1, \"rank\": 0.020191199326071968}, {\"index\": 176, \"text\": \"30x\", \"count\": 1, \"rank\": 0.020171915093913386}, {\"index\": 177, \"text\": \"Stability AI\\u2019s\", \"count\": 1, \"rank\": 0.019976555908326886}, {\"index\": 178, \"text\": \"30K requests\", \"count\": 1, \"rank\": 0.019887225498907982}, {\"index\": 179, \"text\": \"the HF\", \"count\": 1, \"rank\": 0.01986189732625275}, {\"index\": 180, \"text\": \"a limited number\", \"count\": 1, \"rank\": 0.01980184269431209}, {\"index\": 181, \"text\": \"texts\", \"count\": 1, \"rank\": 0.01979022857361993}, {\"index\": 182, \"text\": \"the prompt\", \"count\": 1, \"rank\": 0.019781760538028794}, {\"index\": 183, \"text\": \"the prompts\", \"count\": 1, \"rank\": 0.019781760538028794}, {\"index\": 184, \"text\": \"\\\\     -H \\\"Content-Type: application/json\\\" \\\\\", \"count\": 1, \"rank\": 0.019767046217550804}, {\"index\": 185, \"text\": \"a significant bottleneck\", \"count\": 1, \"rank\": 0.019663807987631243}, {\"index\": 186, \"text\": \"the peak traffic\", \"count\": 1, \"rank\": 0.019425942572304594}, {\"index\": 187, \"text\": \"a small research team\", \"count\": 1, \"rank\": 0.019326530628281925}, {\"index\": 188, \"text\": \"The KV cache\", \"count\": 1, \"rank\": 0.019063179345783745}, {\"index\": 189, \"text\": \"the KV cache\", \"count\": 2, \"rank\": 0.019063179345783745}, {\"index\": 190, \"text\": \"LLaMA\", \"count\": 1, \"rank\": 0.01905862353899243}, {\"index\": 191, \"text\": \"Koala\", \"count\": 2, \"rank\": 0.019055845796028165}, {\"index\": 192, \"text\": \"the new state\", \"count\": 1, \"rank\": 0.019027545240886908}, {\"index\": 193, \"text\": \"a significant challenge\", \"count\": 1, \"rank\": 0.01900945956703393}, {\"index\": 194, \"text\": \"* Equal Contribution\", \"count\": 1, \"rank\": 0.018985489164410892}, {\"index\": 195, \"text\": \"a fixed number\", \"count\": 1, \"rank\": 0.018781968122596453}, {\"index\": 196, \"text\": \"track\", \"count\": 1, \"rank\": 0.01876242986504907}, {\"index\": 197, \"text\": \"]  # Sample prompts\", \"count\": 1, \"rank\": 0.018570334448878574}, {\"index\": 198, \"text\": \"more than half\", \"count\": 2, \"rank\": 0.01836021579587357}, {\"index\": 199, \"text\": \"Paper (Stay Tuned\", \"count\": 1, \"rank\": 0.018255505010340962}, {\"index\": 200, \"text\": \"Performance\", \"count\": 1, \"rank\": 0.018068163154029017}, {\"index\": 201, \"text\": \"May\", \"count\": 1, \"rank\": 0.01806643168164917}, {\"index\": 202, \"text\": \"the above traffic\", \"count\": 1, \"rank\": 0.01801541504635542}, {\"index\": 203, \"text\": \"the keys\", \"count\": 2, \"rank\": 0.01793600777700507}, {\"index\": 204, \"text\": \"a mere waste\", \"count\": 1, \"rank\": 0.017777294169626038}, {\"index\": 205, \"text\": \"the attention computation\", \"count\": 1, \"rank\": 0.017372470661738984}, {\"index\": 206, \"text\": \"order\", \"count\": 1, \"rank\": 0.017299602239495476}, {\"index\": 207, \"text\": \"June 20th\", \"count\": 1, \"rank\": 0.017248050196797794}, {\"index\": 208, \"text\": \"the Chatbot Arena\", \"count\": 2, \"rank\": 0.017151916645483534}, {\"index\": 209, \"text\": \"Blog\", \"count\": 2, \"rank\": 0.017146702667510304}, {\"index\": 210, \"text\": \"#\", \"count\": 2, \"rank\": 0.017078976956404597}, {\"index\": 211, \"text\": \"the chat demo\", \"count\": 1, \"rank\": 0.016917176634540516}, {\"index\": 212, \"text\": \"a single command\", \"count\": 1, \"rank\": 0.0167992078579403}, {\"index\": 213, \"text\": \"Special thanks\", \"count\": 1, \"rank\": 0.01673668747635632}, {\"index\": 214, \"text\": \"LAION\\u2019s OpenAsssiant\", \"count\": 1, \"rank\": 0.016647962908613322}, {\"index\": 215, \"text\": \"under 4%\", \"count\": 2, \"rank\": 0.0164261080068621}, {\"index\": 216, \"text\": \"the system\", \"count\": 1, \"rank\": 0.016358156900763667}, {\"index\": 217, \"text\": \"the growing demands\", \"count\": 1, \"rank\": 0.0163135941278161}, {\"index\": 218, \"text\": \"-m vllm.entrypoints.openai.api_server --model lmsys/vicuna-7b-v1.3\", \"count\": 1, \"rank\": 0.016242184617889167}, {\"index\": 219, \"text\": \"the previous state\", \"count\": 1, \"rank\": 0.01593189987226156}, {\"index\": 220, \"text\": \"a request\", \"count\": 2, \"rank\": 0.015334903285215449}, {\"index\": 221, \"text\": \"each request\", \"count\": 2, \"rank\": 0.015334903285215449}, {\"index\": 222, \"text\": \"the requests\", \"count\": 1, \"rank\": 0.015334903285215449}, {\"index\": 223, \"text\": \"The Secret Sauce: PagedAttention In\", \"count\": 1, \"rank\": 0.015319371325850747}, {\"index\": 224, \"text\": \"a wider range\", \"count\": 1, \"rank\": 0.015186900310613293}, {\"index\": 225, \"text\": \"France\", \"count\": 2, \"rank\": 0.015076337261031055}, {\"index\": 226, \"text\": \"14x - 24x\", \"count\": 1, \"rank\": 0.01491369049194056}, {\"index\": 227, \"text\": \"3.3x - 3.5x\", \"count\": 1, \"rank\": 0.014778539038594003}, {\"index\": 228, \"text\": \"the performance\", \"count\": 2, \"rank\": 0.014562896245119012}, {\"index\": 229, \"text\": \"| Paper (Stay Tuned)   LLMs\", \"count\": 1, \"rank\": 0.014548640967325234}, {\"index\": 230, \"text\": \"vllm  vLLM\", \"count\": 1, \"rank\": 0.014457337355460793}, {\"index\": 231, \"text\": \"the past two months\", \"count\": 2, \"rank\": 0.014401603783140388}, {\"index\": 232, \"text\": \"the following command\", \"count\": 1, \"rank\": 0.014357484738430154}, {\"index\": 233, \"text\": \"the corresponding section\", \"count\": 1, \"rank\": 0.014339125943939419}, {\"index\": 234, \"text\": \"the number\", \"count\": 1, \"rank\": 0.014238968498174473}, {\"index\": 235, \"text\": \"the entire team\", \"count\": 1, \"rank\": 0.014110505094562981}, {\"index\": 236, \"text\": \"the server\", \"count\": 1, \"rank\": 0.01405200955326575}, {\"index\": 237, \"text\": \"LLM(model=\\\"lmsys\", \"count\": 1, \"rank\": 0.01398713808702859}, {\"index\": 238, \"text\": \"the reference counts\", \"count\": 1, \"rank\": 0.013972568552958172}, {\"index\": 239, \"text\": \"your Python scripts\", \"count\": 1, \"rank\": 0.013905544756477907}, {\"index\": 240, \"text\": \"up to 55%\", \"count\": 2, \"rank\": 0.013461910371519668}, {\"index\": 241, \"text\": \"a clear demonstration\", \"count\": 1, \"rank\": 0.013359084602305449}, {\"index\": 242, \"text\": \"the integration\", \"count\": 1, \"rank\": 0.013310920902430157}, {\"index\": 243, \"text\": \"-\", \"count\": 1, \"rank\": 0.013308512011846999}, {\"index\": 244, \"text\": \"2.2x\", \"count\": 1, \"rank\": 0.013308512011846999}, {\"index\": 245, \"text\": \"the classic idea\", \"count\": 1, \"rank\": 0.01329753085835121}, {\"index\": 246, \"text\": \"Team\", \"count\": 1, \"rank\": 0.013187536193880295}, {\"index\": 247, \"text\": \"these tensors\", \"count\": 1, \"rank\": 0.013103805869906752}, {\"index\": 248, \"text\": \"40GB\", \"count\": 1, \"rank\": 0.0128460111744821}, {\"index\": 249, \"text\": \"the art\", \"count\": 2, \"rank\": 0.012802019883114883}, {\"index\": 250, \"text\": \"$ curl http://localhost:8000/v1/completions\", \"count\": 1, \"rank\": 0.012560531018262985}, {\"index\": 251, \"text\": \"all industries\", \"count\": 1, \"rank\": 0.012559126609432439}, {\"index\": 252, \"text\": \"an open-source library\", \"count\": 1, \"rank\": 0.012485538957894731}, {\"index\": 253, \"text\": \"over-reservation\", \"count\": 1, \"rank\": 0.012334748856769436}, {\"index\": 254, \"text\": \"Example\", \"count\": 1, \"rank\": 0.011747970214973282}, {\"index\": 255, \"text\": \"PageAttention\", \"count\": 1, \"rank\": 0.011747970214973282}, {\"index\": 256, \"text\": \"Today\", \"count\": 1, \"rank\": 0.011747970214973282}, {\"index\": 257, \"text\": \"example\", \"count\": 1, \"rank\": 0.011747970214973282}, {\"index\": 258, \"text\": \"mid\", \"count\": 1, \"rank\": 0.011747970214973282}, {\"index\": 259, \"text\": \"practice\", \"count\": 1, \"rank\": 0.011747970214973282}, {\"index\": 260, \"text\": \"{         \\\"model\", \"count\": 1, \"rank\": 0.0114144630638676}, {\"index\": 261, \"text\": \"8.5x - 15x\", \"count\": 1, \"rank\": 0.011240313128528297}, {\"index\": 262, \"text\": \"a peak\", \"count\": 1, \"rank\": 0.011159042157088387}, {\"index\": 263, \"text\": \"the use\", \"count\": 1, \"rank\": 0.011150836255755305}, {\"index\": 264, \"text\": \"The LMSYS\", \"count\": 1, \"rank\": 0.011139511988190793}, {\"index\": 265, \"text\": \"This April\", \"count\": 1, \"rank\": 0.011107607243504831}, {\"index\": 266, \"text\": \"between April to May\", \"count\": 1, \"rank\": 0.010915219736366765}, {\"index\": 267, \"text\": \"This utilization\", \"count\": 1, \"rank\": 0.010850112602557969}, {\"index\": 268, \"text\": \"Databricks Dolly\", \"count\": 1, \"rank\": 0.010546589977799477}, {\"index\": 269, \"text\": \"another key advantage\", \"count\": 1, \"rank\": 0.010519078989473568}, {\"index\": 270, \"text\": \"a more flexible way\", \"count\": 1, \"rank\": 0.010482566713522564}, {\"index\": 271, \"text\": \"60K\", \"count\": 1, \"rank\": 0.010116103879342905}, {\"index\": 272, \"text\": \"the demo\", \"count\": 1, \"rank\": 0.009914615122167458}, {\"index\": 273, \"text\": \"$ pip\", \"count\": 1, \"rank\": 0.00945829480144956}, {\"index\": 274, \"text\": \"to 1.7GB\", \"count\": 1, \"rank\": 0.009328572626993166}, {\"index\": 275, \"text\": \"our paper\", \"count\": 1, \"rank\": 0.009316690561112513}, {\"index\": 276, \"text\": \"the-art\", \"count\": 1, \"rank\": 0.00929662683846003}, {\"index\": 277, \"text\": \"the core technology\", \"count\": 2, \"rank\": 0.009285629710564328}, {\"index\": 278, \"text\": \"June 20th,\", \"count\": 1, \"rank\": 0.009147159942301265}, {\"index\": 279, \"text\": \"a variety\", \"count\": 1, \"rank\": 0.00913399752531888}, {\"index\": 280, \"text\": \"The Secret Sauce\", \"count\": 1, \"rank\": 0.00887596890684864}, {\"index\": 281, \"text\": \"The Silent Hero\", \"count\": 1, \"rank\": 0.00887596890684864}, {\"index\": 282, \"text\": \"up to 2.2x improvement\", \"count\": 1, \"rank\": 0.007520635327310399}, {\"index\": 283, \"text\": \"the computation\", \"count\": 1, \"rank\": 0.006946233890810962}, {\"index\": 284, \"text\": \"an average\", \"count\": 1, \"rank\": 0.006900214608725471}, {\"index\": 285, \"text\": \"2.2x - 2.5x\", \"count\": 1, \"rank\": 0.006881610907321546}, {\"index\": 286, \"text\": \"all the input\", \"count\": 1, \"rank\": 0.0068547409110543255}, {\"index\": 287, \"text\": \"a\\\",         \\\"max_tokens\\\": 7,         \\\"temperature\", \"count\": 1, \"rank\": 0.0066951172349367745}, {\"index\": 288, \"text\": \"The capital\", \"count\": 1, \"rank\": 0.006146938394042587}, {\"index\": 289, \"text\": \"this problem\", \"count\": 1, \"rank\": 0.006146938394042587}, {\"index\": 290, \"text\": \"two settings\", \"count\": 1, \"rank\": 0.006146938394042587}, {\"index\": 291, \"text\": \"All rights\", \"count\": 1, \"rank\": 0.005426155012837224}, {\"index\": 292, \"text\": \"Its size\", \"count\": 1, \"rank\": 0.005426155012837224}, {\"index\": 293, \"text\": \"This boost\", \"count\": 1, \"rank\": 0.005426155012837224}, {\"index\": 294, \"text\": \"a result\", \"count\": 1, \"rank\": 0.005426155012837224}, {\"index\": 295, \"text\": \"my name\", \"count\": 1, \"rank\": 0.005426155012837224}, {\"index\": 296, \"text\": \"our experiments\", \"count\": 1, \"rank\": 0.005426155012837224}, {\"index\": 297, \"text\": \"this case\", \"count\": 1, \"rank\": 0.005426155012837224}, {\"index\": 298, \"text\": \"$\", \"count\": 1, \"rank\": 0.0}, {\"index\": 299, \"text\": \"2023\", \"count\": 2, \"rank\": 0.0}, {\"index\": 300, \"text\": \"30\", \"count\": 1, \"rank\": 0.0}, {\"index\": 301, \"text\": \"40\", \"count\": 1, \"rank\": 0.0}, {\"index\": 302, \"text\": \"60\", \"count\": 1, \"rank\": 0.0}, {\"index\": 303, \"text\": \"7\", \"count\": 1, \"rank\": 0.0}, {\"index\": 304, \"text\": \"It\", \"count\": 2, \"rank\": 0.0}, {\"index\": 305, \"text\": \"LLaMA-7B\", \"count\": 1, \"rank\": 0.0}, {\"index\": 306, \"text\": \"The\", \"count\": 1, \"rank\": 0.0}, {\"index\": 307, \"text\": \"This\", \"count\": 2, \"rank\": 0.0}, {\"index\": 308, \"text\": \"We\", \"count\": 5, \"rank\": 0.0}, {\"index\": 309, \"text\": \"You\", \"count\": 1, \"rank\": 0.0}, {\"index\": 310, \"text\": \"daily\", \"count\": 1, \"rank\": 0.0}, {\"index\": 311, \"text\": \"http://localhost:8000\", \"count\": 1, \"rank\": 0.0}, {\"index\": 312, \"text\": \"it\", \"count\": 1, \"rank\": 0.0}, {\"index\": 313, \"text\": \"one\", \"count\": 2, \"rank\": 0.0}, {\"index\": 314, \"text\": \"our\", \"count\": 3, \"rank\": 0.0}, {\"index\": 315, \"text\": \"that\", \"count\": 4, \"rank\": 0.0}, {\"index\": 316, \"text\": \"the\", \"count\": 1, \"rank\": 0.0}, {\"index\": 317, \"text\": \"them\", \"count\": 1, \"rank\": 0.0}, {\"index\": 318, \"text\": \"this\", \"count\": 1, \"rank\": 0.0}, {\"index\": 319, \"text\": \"three\", \"count\": 1, \"rank\": 0.0}, {\"index\": 320, \"text\": \"two\", \"count\": 1, \"rank\": 0.0}, {\"index\": 321, \"text\": \"up to 5x\", \"count\": 1, \"rank\": 0.0}, {\"index\": 322, \"text\": \"we\", \"count\": 5, \"rank\": 0.0}, {\"index\": 323, \"text\": \"which\", \"count\": 2, \"rank\": 0.0}, {\"index\": 324, \"text\": \"you\", \"count\": 2, \"rank\": 0.0}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = doc._.textrank\n",
    "tr.plot_keyphrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "en_nlp.add_pipe(\"textrank\", config={ \"stopwords\": { \"word\": [\"NOUN\"] } })\n",
    "doc = en_nlp(text)\n",
    "tr = doc._.textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM achieves 8.5x - 15x higher throughput than HF and 3.3x - 3.5x higher throughput than TGI.  \n",
      "Since mid-April, the most popular models such as Vicuna, Koala, and LLaMA, have all been successfully served using the FastChat-vLLM integration – With FastChat as the multi-model chat serving frontend and vLLM as the inference backend, LMSYS is able to harness a limited number of university-sponsored GPUs to serve Vicuna to millions of users with high throughput and low latency.\n"
     ]
    }
   ],
   "source": [
    "for sent in tr.summary(limit_phrases=10, limit_sentences=2):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import summarizer\n",
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention                                vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention By Woosuk Kwon*, Zhuohan Li*, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Yu, Joey Gonzalez, Hao Zhang, and Ion Stoica (* Equal Contribution).\n",
      "vLLM utilizes PagedAttention, our new attention algorithm that effectively manages attention keys and values.\n",
      "vLLM equipped with PagedAttention redefines the new state of the art in LLM serving: it delivers up to 24x higher throughput than HuggingFace Transformers, without requiring any model architecture changes.\n",
      "The Secret Sauce: PagedAttention In vLLM, we identify that the performance of LLM serving is bottlenecked by memory.\n",
      "Specifically, PagedAttention partitions the KV cache of each sequence into blocks, each block containing the keys and values for a fixed number of tokens.\n",
      "Because the blocks do not need to be contiguous in memory, we can manage the keys and values in a more flexible way as in OS’s virtual memory: one can think of blocks as pages, tokens as bytes, and sequences as processes.\n",
      "In PagedAttention, memory waste only happens in the last block of a sequence.\n",
      "PagedAttention is the core technology behind vLLM, our LLM inference and serving engine that supports a variety of models with high performance and an easy-to-use interface.\n",
      "The LMSYS and vLLM team have worked together and soon developed the FastChat-vLLM integration to use vLLM  in order to support the growing demands (up to 5x more traffic).\n",
      "In an early  by LMSYS, the vLLM serving backend can achieve up to 30x higher throughput than an initial HF backend.\n",
      "Since mid-April, the most popular models such as Vicuna, Koala, and LLaMA, have all been successfully served using the FastChat-vLLM integration – With FastChat as the multi-model chat serving frontend and vLLM as the inference backend, LMSYS is able to harness a limited number of university-sponsored GPUs to serve Vicuna to millions of users with high throughput and low latency.\n",
      "Requests served by FastChat-vLLM integration in the Chatbot Arena between April to May. Indeed, more than half of the requests to Chatbot Arena use vLLM as the inference backend.\n",
      "With vLLM, LMSYS was able to cut the number of GPUs used for serving the above traffic by 50%.\n",
      "To use vLLM for online serving, you can start an OpenAI API-compatible server via: $ python -m vllm.entrypoints.openai.api_server --model lmsys/vicuna-7b-v1.3  You can query the server with the same format as OpenAI API: $ curl http://localhost:8000/v1/completions \\     -H \"Content-Type: application/json\" \\     -d '{         \"model\": \"lmsys/vicuna-7b-v1.3\",         \"prompt\": \"San Francisco is a\",         \"max_tokens\": 7,         \"temperature\": 0     }'  For more ways to use vLLM, please check out the .\n"
     ]
    }
   ],
   "source": [
    "print(summarizer.summarize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2811"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = summarizer.summarize(text)\n",
    "len(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vllm\n",
      "pagedattention\n",
      "serving\n",
      "served\n",
      "serve\n",
      "llm\n",
      "memory\n",
      "models\n",
      "model\n",
      "lmsys\n",
      "output\n",
      "outputs\n",
      "requests\n",
      "request\n",
      "sample\n",
      "sampling\n",
      "samples\n",
      "efficiently\n",
      "efficiency\n",
      "efficient\n",
      "berkeley\n",
      "uc\n",
      "prompt\n",
      "prompts\n",
      "sharing\n",
      "shared\n",
      "share\n",
      "vicuna\n",
      "highly\n",
      "high\n",
      "gpu\n",
      "significant\n",
      "significantly\n",
      "generate\n",
      "generated\n",
      "generation\n",
      "sequence\n",
      "sequences\n",
      "blocks\n",
      "block\n",
      "research team\n",
      "new attention algorithm\n",
      "use\n",
      "systems waste\n",
      "hardware\n",
      "traffic\n",
      "search\n",
      "researcher\n",
      "keys\n",
      "key\n",
      "throughput\n",
      "transformers\n",
      "algorithms\n",
      "contiguous\n",
      "backend\n",
      "completion\n",
      "completions\n",
      "operating\n",
      "compute\n",
      "computation\n",
      "april\n",
      "developed\n",
      "api\n",
      "reduced operational\n",
      "stoica\n",
      "physical\n",
      "command\n",
      "performance\n",
      "reduces\n",
      "cached\n",
      "cache\n",
      "hf\n",
      "llms\n",
      "initially\n",
      "initial\n",
      "li\n",
      "openai\n",
      "siyuan\n"
     ]
    }
   ],
   "source": [
    "print(keywords.keywords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PytextRank](https://xang1234.github.io/textrank/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixed types \t\t\t\t\t\trank= 0.18359439311764025 \t#count= 1\n",
      "systems \t\t\t\t\t\trank= 0.1784796193107821 \t#count= 3\n",
      "minimal generating sets \t\t\t\t\t\trank= 0.15037838042245094 \t#count= 1\n",
      "nonstrict inequations \t\t\t\t\t\trank= 0.14740065982407313 \t#count= 1\n",
      "strict inequations \t\t\t\t\t\trank= 0.13946027725597837 \t#count= 1\n",
      "linear Diophantine equations \t\t\t\t\t\trank= 0.1195023546245721 \t#count= 1\n",
      "natural numbers \t\t\t\t\t\trank= 0.11450088293222845 \t#count= 1\n",
      "solutions \t\t\t\t\t\trank= 0.10780718173686318 \t#count= 3\n",
      "linear constraints \t\t\t\t\t\trank= 0.10529828014583348 \t#count= 1\n",
      "all the considered types systems \t\t\t\t\t\trank= 0.1036960590708142 \t#count= 1\n",
      "a minimal supporting set \t\t\t\t\t\trank= 0.08812713074893187 \t#count= 1\n",
      "linear \t\t\t\t\t\trank= 0.08444534702772151 \t#count= 1\n",
      "a system \t\t\t\t\t\trank= 0.08243620500315359 \t#count= 1\n",
      "a minimal set \t\t\t\t\t\trank= 0.07944607954086784 \t#count= 1\n",
      "algorithms \t\t\t\t\t\trank= 0.0763527926213032 \t#count= 1\n",
      "all types \t\t\t\t\t\trank= 0.07593126037016427 \t#count= 1\n",
      "Diophantine \t\t\t\t\t\trank= 0.07309361902551355 \t#count= 1\n",
      "construction \t\t\t\t\t\trank= 0.0702090100898443 \t#count= 1\n",
      "the set \t\t\t\t\t\trank= 0.05800111772673988 \t#count= 1\n",
      "components \t\t\t\t\t\trank= 0.054251394765316464 \t#count= 1\n",
      "Compatibility \t\t\t\t\t\trank= 0.04516904342912139 \t#count= 1\n",
      "compatibility \t\t\t\t\t\trank= 0.04516904342912139 \t#count= 1\n",
      "the corresponding algorithms \t\t\t\t\t\trank= 0.04435648606848154 \t#count= 1\n",
      "Criteria \t\t\t\t\t\trank= 0.042273783712246285 \t#count= 1\n",
      "These criteria \t\t\t\t\t\trank= 0.01952542432474353 \t#count= 1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "# example text\n",
    "text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# examine the top-ranked phrases in the document\n",
    "for phrase in doc._.phrases:\n",
    "    print(phrase.text,f'\\t\\t\\t\\t\\t\\trank= {phrase.rank}', f'\\t#count= {phrase.count}')\n",
    "    # print(phrase.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance \t\t\t\t\t\trank= 0.0878734206449319 \t#count= 1\n",
      "instruction datasets \t\t\t\t\t\trank= 0.0843157687536305 \t#count= 1\n",
      "chat models \t\t\t\t\t\trank= 0.08372665445890692 \t#count= 1\n",
      "Benjamin Marie | Aug \t\t\t\t\t\trank= 0.08365595129214401 \t#count= 1\n",
      "GB \t\t\t\t\t\trank= 0.07668777993495272 \t#count= 3\n",
      "quantization \t\t\t\t\t\trank= 0.06547780713335308 \t#count= 3\n",
      "Fast Inference \t\t\t\t\t\trank= 0.06527731804561555 \t#count= 2\n",
      "Benjamin Marie \t\t\t\t\t\trank= 0.06494313741345766 \t#count= 1\n",
      "Benjamin Mariein \t\t\t\t\t\trank= 0.06479504490826045 \t#count= 2\n",
      "AI \t\t\t\t\t\trank= 0.06462252371855874 \t#count= 3\n",
      "in-- \t\t\t\t\t\trank= 0.06322243979666593 \t#count= 1\n",
      "Hugging Face Hub \t\t\t\t\t\trank= 0.061217387293784134 \t#count= 2\n",
      "LLM \t\t\t\t\t\trank= 0.06109173230265478 \t#count= 1\n",
      "LLMs \t\t\t\t\t\trank= 0.06109173230265478 \t#count= 3\n",
      "Llama \t\t\t\t\t\trank= 0.05885078378334063 \t#count= 3\n",
      "quantizing it?No \t\t\t\t\t\trank= 0.05794155651555871 \t#count= 1\n",
      "its original size \t\t\t\t\t\trank= 0.057433633818014036 \t#count= 1\n",
      "Hugging Face \t\t\t\t\t\trank= 0.0567545154650728 \t#count= 1\n",
      "most standard computers \t\t\t\t\t\trank= 0.05337557654893878 \t#count= 1\n",
      "Listsin----in \t\t\t\t\t\trank= 0.04943822912432088 \t#count= 1\n",
      "memory \t\t\t\t\t\trank= 0.048912762297200534 \t#count= 2\n",
      "QLoRa \t\t\t\t\t\trank= 0.04879612271744919 \t#count= 2\n",
      "consumer hardware \t\t\t\t\t\trank= 0.04821679079879699 \t#count= 1\n",
      "GTPQ \t\t\t\t\t\trank= 0.048152264837981185 \t#count= 4\n",
      "the entire model \t\t\t\t\t\trank= 0.04777352787956602 \t#count= 2\n",
      "RAM \t\t\t\t\t\trank= 0.04760907273000171 \t#count= 2\n",
      "Medium \t\t\t\t\t\trank= 0.047215600671301226 \t#count= 1\n",
      "most machines \t\t\t\t\t\trank= 0.04716731041345843 \t#count= 1\n",
      "Mediumin \t\t\t\t\t\trank= 0.045388933729452775 \t#count= 2\n",
      "a Hugging Face account \t\t\t\t\t\trank= 0.04438609758090395 \t#count= 1\n",
      "in----in \t\t\t\t\t\trank= 0.04303274572426163 \t#count= 1\n",
      "MediumMember \t\t\t\t\t\trank= 0.042866327767841335 \t#count= 1\n",
      "Aug \t\t\t\t\t\trank= 0.04276156022625659 \t#count= 1\n",
      "its size \t\t\t\t\t\trank= 0.0423517042282848 \t#count= 1\n",
      "7B parameters \t\t\t\t\t\trank= 0.04068907130533532 \t#count= 1\n",
      "a quantization algorithm \t\t\t\t\t\trank= 0.039898326674263074 \t#count= 1\n",
      "FollowMore \t\t\t\t\t\trank= 0.03946736327468421 \t#count= 2\n",
      "Technology \t\t\t\t\t\trank= 0.039109955287679014 \t#count= 1\n",
      "the model \t\t\t\t\t\trank= 0.03891533818971855 \t#count= 3\n",
      "your model \t\t\t\t\t\trank= 0.03891533818971855 \t#count= 1\n",
      "your computer·3 min read·Aug \t\t\t\t\t\trank= 0.0386788204345359 \t#count= 1\n",
      "AI and Technology \t\t\t\t\t\trank= 0.03776790692625135 \t#count= 1\n",
      "fine-tuning \t\t\t\t\t\trank= 0.037355187169238344 \t#count= 1\n",
      "NLP/AI \t\t\t\t\t\trank= 0.036533989091453035 \t#count= 2\n",
      "Meta \t\t\t\t\t\trank= 0.03640311320259068 \t#count= 4\n",
      "research scientist \t\t\t\t\t\trank= 0.035924635394101914 \t#count= 1\n",
      "an open LLM \t\t\t\t\t\trank= 0.03578025157773541 \t#count= 1\n",
      "13.5 GB \t\t\t\t\t\trank= 0.035420568311199795 \t#count= 1\n",
      "3.6 GB \t\t\t\t\t\trank= 0.035420568311199795 \t#count= 1\n",
      "a Google Colab Pro instance \t\t\t\t\t\trank= 0.03470744627833472 \t#count= 1\n",
      "Llama 2 WeightsLlama \t\t\t\t\t\trank= 0.03330888918399844 \t#count= 1\n",
      "GPTQ \t\t\t\t\t\trank= 0.0332681390701987 \t#count= 11\n",
      "26.6% \t\t\t\t\t\trank= 0.03275996363477067 \t#count= 1\n",
      "inference \t\t\t\t\t\trank= 0.032361253813495514 \t#count= 1\n",
      "the same email \t\t\t\t\t\trank= 0.0321490394476129 \t#count= 1\n",
      "Exclusive articles \t\t\t\t\t\trank= 0.03171183642854901 \t#count= 1\n",
      "Fast Inference on Your Computer \t\t\t\t\t\trank= 0.029992671294772875 \t#count= 1\n",
      "this account \t\t\t\t\t\trank= 0.02891155388850974 \t#count= 1\n",
      "your account \t\t\t\t\t\trank= 0.02891155388850974 \t#count= 1\n",
      "the instructions \t\t\t\t\t\trank= 0.028698161830667774 \t#count= 1\n",
      "7B \t\t\t\t\t\trank= 0.02856066585958352 \t#count= 1\n",
      "Your ComputerThe power \t\t\t\t\t\trank= 0.02785557030346322 \t#count= 1\n",
      "2023 | MediumMember-only storyQuantization \t\t\t\t\t\trank= 0.02776958434669892 \t#count= 1\n",
      "The email address \t\t\t\t\t\trank= 0.02759828138864824 \t#count= 1\n",
      "the layer level \t\t\t\t\t\trank= 0.027555724915021455 \t#count= 1\n",
      "Llama 2 \t\t\t\t\t\trank= 0.027182012687465743 \t#count= 3\n",
      "all my AI notebooks \t\t\t\t\t\trank= 0.0269105486457185 \t#count= 1\n",
      "a significant drop \t\t\t\t\t\trank= 0.02641773419021618 \t#count= 1\n",
      "a Google Colab Pro \t\t\t\t\t\trank= 0.026135025066083905 \t#count= 1\n",
      "CPU \t\t\t\t\t\trank= 0.025996902393592484 \t#count= 1\n",
      "4-bit quantization \t\t\t\t\t\trank= 0.025096545177817876 \t#count= 1\n",
      "the Llama 2 weights \t\t\t\t\t\trank= 0.024661097876329946 \t#count= 1\n",
      "Medium \"Top writer \t\t\t\t\t\trank= 0.02255445384459662 \t#count= 1\n",
      "D \t\t\t\t\t\trank= 0.022259341889112702 \t#count= 1\n",
      "an email \t\t\t\t\t\trank= 0.02218052874615853 \t#count= 1\n",
      "On Medium \t\t\t\t\t\trank= 0.02180795180602051 \t#count= 1\n",
      "at least 32 GB \t\t\t\t\t\trank= 0.021080186421354178 \t#count= 1\n",
      "one hour \t\t\t\t\t\trank= 0.020693972894908347 \t#count= 2\n",
      "i.e., 26.6% \t\t\t\t\t\trank= 0.019496754950693147 \t#count= 1\n",
      "the best option \t\t\t\t\t\trank= 0.019051914974117793 \t#count= 1\n",
      "Your Computer \t\t\t\t\t\trank= 0.01790733503131152 \t#count= 1\n",
      "a few words \t\t\t\t\t\trank= 0.01778767102441876 \t#count= 1\n",
      "5--SharePhoto \t\t\t\t\t\trank= 0.015437050628552324 \t#count= 1\n",
      "a very clever (and complicated) algorithm \t\t\t\t\t\trank= 0.013667218241719263 \t#count= 1\n",
      "The 7 billion parameter version \t\t\t\t\t\trank= 0.012114276316179333 \t#count= 1\n",
      "the CPU \t\t\t\t\t\trank= 0.012007454876029638 \t#count= 1\n",
      "a way \t\t\t\t\t\trank= 0.011646960938485973 \t#count= 1\n",
      "an LLM \t\t\t\t\t\trank= 0.011644642981485931 \t#count= 1\n",
      "this scenario \t\t\t\t\t\trank= 0.010281149625330506 \t#count= 1\n",
      "13.5 \t\t\t\t\t\trank= 0.0 \t#count= 1\n",
      "2023 \t\t\t\t\t\trank= 0.0 \t#count= 1\n",
      "3.6 \t\t\t\t\t\trank= 0.0 \t#count= 1\n",
      "4 \t\t\t\t\t\trank= 0.0 \t#count= 1\n",
      "5 \t\t\t\t\t\trank= 0.0 \t#count= 1\n",
      "7 billion \t\t\t\t\t\trank= 0.0 \t#count= 1\n",
      "I \t\t\t\t\t\trank= 0.0 \t#count= 2\n",
      "It \t\t\t\t\t\trank= 0.0 \t#count= 3\n",
      "This \t\t\t\t\t\trank= 0.0 \t#count= 1\n",
      "You \t\t\t\t\t\trank= 0.0 \t#count= 3\n",
      "a \t\t\t\t\t\trank= 0.0 \t#count= 1\n",
      "at least 32 \t\t\t\t\t\trank= 0.0 \t#count= 1\n",
      "it \t\t\t\t\t\trank= 0.0 \t#count= 4\n",
      "that \t\t\t\t\t\trank= 0.0 \t#count= 3\n",
      "we \t\t\t\t\t\trank= 0.0 \t#count= 4\n",
      "what \t\t\t\t\t\trank= 0.0 \t#count= 1\n",
      "you \t\t\t\t\t\trank= 0.0 \t#count= 5\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "from basic_cleaner import clean\n",
    "\n",
    "# example text\n",
    "text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\n",
    "url= 'https://medium.com/@bnjmn_marie/quantization-of-llama-2-with-gtpq-for-fast-inference-on-your-computer-a6eff6ccde59'\n",
    "text= clean(url)\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# examine the top-ranked phrases in the document\n",
    "# for phrase in doc._.phrases:\n",
    "#     print(phrase.text,f'\\t\\t\\t\\t\\t\\trank= {phrase.rank}', f'\\t#count= {phrase.count}')\n",
    "    # print(phrase.chunks)\n",
    "\n",
    "\n",
    "with open('keywords.csv', 'w') as f:\n",
    "    f.write('keywords, rank, frequency\\n')\n",
    "    for phrase in doc._.phrases:\n",
    "        print(phrase.text,f'\\t\\t\\t\\t\\t\\trank= {phrase.rank}', f'\\t#count= {phrase.count}')\n",
    "        f.write(f\"{phrase.text},{phrase.rank}, {phrase.count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
